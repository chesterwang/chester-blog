<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Huggingfaceåº“å­¦ä¹ ç¬”è®° | Chester&#39;s Blog</title>
<meta name="keywords" content="LLM, HuggingFace">
<meta name="description" content="Huggingfaceç›¸å…³Pythonåº“å­¦ä¹ ç¬”è®°">
<meta name="author" content="ä½œè€…:Chester">
<link rel="canonical" href="https://chesterwang.github.io/chester-blog/posts/2025-08-22-huggingface-%E7%B1%BB%E5%BA%93%E5%AD%A6%E4%B9%A0/">
<link crossorigin="anonymous" href="/chester-blog/assets/css/stylesheet.3d787722632b03b0bffe0d67e8f4333c75f6538432f31f18a83dfd05a3aeae4b.css" integrity="sha256-PXh3ImMrA7C//g1n6PQzPHX2U4Qy8x8YqD39BaOurks=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/chester-blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://image.lvbibir.cn/blog/avatar.webp">
<link rel="icon" type="image/png" sizes="16x16" href="https://image.lvbibir.cn/blog/avatar.webp">
<link rel="icon" type="image/png" sizes="32x32" href="https://image.lvbibir.cn/blog/avatar.webp">
<link rel="apple-touch-icon" href="https://image.lvbibir.cn/blog/avatar.webp">
<link rel="mask-icon" href="https://image.lvbibir.cn/blog/avatar.webp">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://chesterwang.github.io/chester-blog/posts/2025-08-22-huggingface-%E7%B1%BB%E5%BA%93%E5%AD%A6%E4%B9%A0/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = ""; 
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




 
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css" integrity="sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js" integrity="sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<meta property="og:title" content="Huggingfaceåº“å­¦ä¹ ç¬”è®°" />
<meta property="og:description" content="Huggingfaceç›¸å…³Pythonåº“å­¦ä¹ ç¬”è®°" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chesterwang.github.io/chester-blog/posts/2025-08-22-huggingface-%E7%B1%BB%E5%BA%93%E5%AD%A6%E4%B9%A0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-08-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-08-22T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Huggingfaceåº“å­¦ä¹ ç¬”è®°"/>
<meta name="twitter:description" content="Huggingfaceç›¸å…³Pythonåº“å­¦ä¹ ç¬”è®°"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "ğŸ“š æ–‡ç« ",
      "item": "https://chesterwang.github.io/chester-blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Huggingfaceåº“å­¦ä¹ ç¬”è®°",
      "item": "https://chesterwang.github.io/chester-blog/posts/2025-08-22-huggingface-%E7%B1%BB%E5%BA%93%E5%AD%A6%E4%B9%A0/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Huggingfaceåº“å­¦ä¹ ç¬”è®°",
  "name": "Huggingfaceåº“å­¦ä¹ ç¬”è®°",
  "description": "Huggingfaceç›¸å…³Pythonåº“å­¦ä¹ ç¬”è®°",
  "keywords": [
    "LLM", "HuggingFace"
  ],
  "articleBody": "Hugging Face Hub Models, Spaces, and Datasets are hosted on the Hugging Face Hub asÂ Git repositories Do you have files larger than 10MB? Those files should be tracked withÂ git-lfs, which you can initialize with: git lfs install Note that if your files are larger thanÂ 5GBÂ youâ€™ll also need to run: hf lfs-enable-largefiles . Pull Request ä¹‹æ‰€ä»¥å«è¿™ä¸ªåå­—ï¼Œæ˜¯å› ä¸ºå®ƒå‡†ç¡®åœ°æè¿°äº†è¯·æ±‚è€…ï¼ˆä½ ï¼‰å’Œè¢«è¯·æ±‚è€…ï¼ˆé¡¹ç›®ç»´æŠ¤è€…ï¼‰ä¹‹é—´çš„åŠ¨ä½œå’Œæ–¹å‘ã€‚ PR = â€œæˆ‘æ”¹å¥½äº†ï¼Œè¯·ä½ æŠŠæˆ‘è¿™è¾¹çš„ä¿®æ”¹æ‹‰ï¼ˆpullï¼‰è¿›ä½ çš„ä¸»åˆ†æ”¯å§ã€‚â€è¿™ä¸ª â€œpullâ€ å¹¶ä¸æ˜¯æŒ‡ä½ è‡ªå·±å»æ‹‰ï¼Œè€Œæ˜¯ è¯·æ±‚é¡¹ç›®ç»´æŠ¤è€…å»æ‹‰ä½ çš„ä»£ç ã€‚ Templates transformers quickstart load a pretrained model run inference withÂ Pipeline fine-tune a model withÂ Trainer Auto Classes Instantiating one ofÂ AutoConfig,Â AutoModel, andÂ AutoTokenizerÂ will directly create a class of the relevant architecture. transformersåº“ä¸­ AutoImageProcessor å®ä¾‹è¯å‡ºæ¥çš„processor çš„ä½œç”¨éƒ½æœ‰å“ªäº› ä¸€æ—¦ä½ é€šè¿‡ AutoImageProcessor.from_pretrained() å®ä¾‹åŒ–äº†ä¸€ä¸ªå¤„ç†å™¨ï¼Œè¿™ä¸ª processor å®ä¾‹ å°±æˆä¸ºäº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œä¸“é—¨ç”¨äºå‡†å¤‡å›¾åƒæ•°æ®ï¼Œä½¿å…¶èƒ½å¤Ÿè¢«ç‰¹å®šçš„é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ä½¿ç”¨ã€‚å®ƒçš„ä¸»è¦ä½œç”¨å¯ä»¥æ¦‚æ‹¬ä¸ºä»¥ä¸‹å‡ ç‚¹ï¼š å›¾åƒæ ‡å‡†åŒ– (Normalization) å›¾åƒå°ºå¯¸è°ƒæ•´ (Resizing) å’Œè£å‰ª (Cropping) é€šé“æ ¼å¼è½¬æ¢ (Channel Format Conversion) å›¾åƒåˆ°å¼ é‡è½¬æ¢ (Image to Tensor Conversion) æ‰¹å¤„ç† (Batching) æ•°æ®å¢å¼º (Data Augmentation) transoformeråº“ä¸­ TFAutoModelå’ŒAutoModelçš„åŒºåˆ«æ˜¯ä»€ä¹ˆ AutoModelï¼šç”¨äºåŠ è½½ PyTorch æ¡†æ¶ä¸‹çš„æ¨¡å‹ã€‚ TFAutoModelï¼šç”¨äºåŠ è½½ TensorFlow 2.0 æ¡†æ¶ä¸‹çš„æ¨¡å‹ã€‚ FlaxAutoModel: ç”¨äºåŠ è½½ Flaxï¼ˆåŸºäº JAX çš„æ¡†æ¶ï¼‰ä¸‹çš„æ¨¡å‹ã€‚ AutoModel ç±»çš„åç¼€ LM CausalLM MaskedLM MaskedGeneration sequenceClassification TokenClassification NextSentencePrediction MultipleChoice Seq2SeqLM QuestionAnswering Backbone A backbone is a model used for feature extraction for higher level computer vision tasks such as object detection and image classification. Data Collator Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements ofÂ train_datasetÂ orÂ eval_dataset. æˆ‘ç†è§£ Data Collator å°±æ˜¯æŠŠå¤šä¸ªåˆ—æ”¾åœ¨ä¸€èµ·ï¼Œå°±æ˜¯collatorçš„å­—é¢æ„æ€ï¼Œä½†å…¶å®å†…éƒ¨è¿˜æ˜¯ä¼šæœ‰ä¸€äº›å…·ä½“çš„æ•°æ®å¤„ç†é€»è¾‘ï¼Œæ¯”å¦‚paddingã€æ•°æ®æ‰©å¢ç­‰ pipeline from transformers import pipeline generator = pipeline(model=\"openai-community/gpt2\") generator(\"I can't believe you did such a \", do_sample=False) [{'generated_text': \"I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I\"}] chat with models Templates Diffusers LoRA Add a LoRA to a pipeline with theÂ load_lora_weights()Â method. Some LoRAâ€™s require a special word to trigger it, such asÂ Realism, in the example below. Check a LoRAâ€™s model card to see if it requires a trigger word. LoRAæ–‡ä»¶å°±æ˜¯ä¸€ç§æ’ä»¶ Sentence Transformers sentence embedding demo\nfrom sentence_transformers import SentenceTransformer # 1. Load a pretrained Sentence Transformer model model = SentenceTransformer(\"all-MiniLM-L6-v2\") # The sentences to encode sentences = [ \"The weather is lovely today.\", \"It's so sunny outside!\", \"He drove to the stadium.\", ] # 2. Calculate embeddings by calling model.encode() embeddings = model.encode(sentences) print(embeddings.shape) # [3, 384] # 3. Calculate the embedding similarities similarities = model.similarity(embeddings, embeddings) print(similarities) # tensor([[1.0000, 0.6660, 0.1046], # [0.6660, 1.0000, 0.1411], # [0.1046, 0.1411, 1.0000]]) trl Quickstart ç®€å•çš„helloworldç¨‹åºã€‚ evaluation: The important thing is that this process should yield a scalar value for each query/response pair. è¿™é‡Œçš„ä¾‹å­ä»£ç åº”è¯¥å·²ç»è¿‡æ—¶äº†ï¼ŒPPOTrainerç°åœ¨æ²¡æœ‰stepè¿™ä¸ªæ–¹æ³•ã€‚ Dataset formats and types How-to guides customozing the training Memory efficient fine-tuning by sharing layers PPO PPOç­–ç•¥ä¸­çš„ä¸€äº›åŸºç¡€çŸ¥è¯† æ·±åº¦å­¦ä¹ ç¡¬ä»¶é…ç½®ä¸­çš„æ¦‚å¿µ device rank world_size node åˆ†åˆ«æ˜¯æŒ‡ä»€ä¹ˆ è¿™å››ä¸ªæ¦‚å¿µçš„å…³ç³»å¯ä»¥æ¦‚æ‹¬ä¸ºï¼šnode æ˜¯ç‰©ç†æœºå™¨ã€‚ ä¸€ä¸ª node å¯ä»¥åŒ…å«å¤šä¸ª deviceï¼Œdeviceé€šå¸¸æ˜¯æŒ‡GPUã€‚ æ¯ä¸ª device é€šå¸¸ç”±ä¸€ä¸ªç‹¬ç«‹çš„ rank è¿›ç¨‹æ¥æ§åˆ¶ã€‚ æ‰€æœ‰ rank è¿›ç¨‹çš„æ€»æ•°å°±æ˜¯ world_sizeã€‚ PPOç¤ºä¾‹ trl/examples/scripts/ppo/ppo.py at v0.21.0 Â· huggingface/trl model=policy, ref_model=ref_policy, reward_model=reward_model, value_model=value_model,ã€ reward_modelå…¶å®æ˜¯æ¯ä¸€å°æ­¥å†³ç­–çš„å³æ—¶çš„ã€ç›´æ¥å¥–åŠ±ï¼Œvalue_modelå¯¹æ¯ä¸€å°æ­¥å†³ç­–çš„å…¨å±€æ€§ã€é•¿æœŸæ€§åæœè¿›è¡Œé¢„æµ‹ã€‚ advantage æˆ‘ç›´è§‚ç†è§£æ˜¯å› ä¸ºQ(s,a) è¿™ä¸ªstateçš„éšæœºé€‰æ‹©å¯¼è‡´åç»­çš„æ‰€æœ‰rewardéƒ½æ¯”è¾ƒå¼‚å¸¸ï¼Œæ‰€ä»¥è¦æ¶ˆé™¤ä¸€éƒ¨åˆ†stateçš„éšæœºæ€§ï¼Œæ‰€ä»¥å‡å»stateçš„valueï¼›å¦‚æœè®­ç»ƒæ•°æ®ä¸­çš„Q(s,a)ä¸­çš„sè¶³å¤Ÿå¤šï¼Œé‚£ä¹ˆå°±å¯ä»¥ç›´æ¥ä½¿ç”¨Qï¼Œä½†å› ä¸ºä¸å¤Ÿå¤šæœ‰äº†éšæœºæ€§ï¼Œæ‰€ä»¥Qä»£è¡¨çš„returnåˆ†å¸ƒå°±åç¦»äº†æ‰€è°“çš„å…¨å±€æ€§çš„Qçš„åˆ†å¸ƒï¼Œæ‰€ä»¥é€šè¿‡å‡å»è¿™ä¸ªsçš„éšæœºæ€§ä»è€Œæ‹‰å›æ­£å¸¸åˆ†å¸ƒã€‚ Mixin ç±»æœ¬èº«ä¸èƒ½ç‹¬ç«‹å®ä¾‹åŒ–ï¼Œå®ƒä¸æ˜¯ç”¨æ¥åˆ›å»ºå¯¹è±¡çš„ã€‚ç›¸åï¼Œå®ƒåƒä¸€ä¸ªâ€œåŠŸèƒ½åŒ…â€æˆ–â€œèƒ½åŠ›æ’ä»¶â€ï¼Œä¸“é—¨è®¾è®¡ç”¨æ¥è¢«å…¶ä»–ç±»ç»§æ‰¿ã€‚å½“ä¸€ä¸ªç±»ç»§æ‰¿äº†ä¸€ä¸ªæˆ–å¤šä¸ª Mixin ç±»æ—¶ï¼Œå®ƒå°±è‡ªåŠ¨è·å¾—äº†è¿™äº› Mixin ç±»æ‰€å®šä¹‰çš„æ‰€æœ‰æ–¹æ³•å’Œå±æ€§ã€‚ PPOä¸­çš„æ•°æ®å¤„ç† transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus æ–¹æ³•ä¸­åŒ…å«äº† tokenizerä¸­çš„paddingå¤„ç†ã€‚ PPOå®˜æ–¹ä¾‹å­ä¸­ä¸­çš„ä»£ç ï¼Œ ä¸ºä»€ä¹ˆ padding_side å’Œåé¢çš„padding=false ï¼Œè¿™ä¸¤è€…ä¸å†²çªä¹ˆ ä»£ç ä¸­ä¸¤ä¸ªåœ°æ–¹ä½¿ç”¨äº†tokenizerï¼Œ PPOTrainer( args=training_args, processing_class=tokenizer,...) è¿™é‡Œçš„tokenizerå®šä¹‰ä¸­æ˜¯å¸¦æœ‰padding=â€œleft\"çš„ã€‚ è¿™é‡Œçš„tokenizer åœ¨PPOTrainerä¸­æ˜¯ä½œä¸º processing_class çš„ä½œç”¨ã€‚ åœ¨PPOTrainerä¸­ ä¼šæ ¹æ® processing_class ç”Ÿæˆä¸€ä¸ª DataCollatorWithPadding ç±»ç”¨äºæ•°æ®è¿›è¡Œå¤„ç†ï¼Œè€Œè¯¥ç±»çš„è®¡ç®—é€»è¾‘æ˜¯ä½¿ç”¨tokenizerå¯¹æ•°æ®ä»…ä»…è¿›è¡Œpaddingæ“ä½œï¼ˆè¿™é‡Œçš„paddingæ˜¯å³å¯¹é½ï¼‰ï¼Œè¿™é‡Œä¸åŒ…æ‹¬encodingæ“ä½œã€‚å› ä¸ºè®­ç»ƒæ•°æ®å·²ç»æ ¹æ®ç¬¬äºŒæ¡ä¸­çš„ prepare_datasetè¿›è¡Œäº†encodingã€‚ prepare_dataset ä¸­çš„ tokenizeå‡½æ•°ã€‚ è¯¥å‡½æ•°çš„è¾“å…¥æ˜¯ä¸€æ¡æ•°æ®ï¼Œæ‰€ä»¥ä¸éœ€è¦è¿›è¡Œpaddingã€‚ æ€»ç»“ï¼š å³encoddingå’Œpaddingæ˜¯åˆ†åœ¨ä¸¤ä¸ªåœ°æ–¹è¿›è¡Œå¤„ç†çš„ï¼Œåˆ†åˆ«æ˜¯PPOTrainerå®ä¾‹åŒ–ä¹‹å‰ å’Œ PPOTrainerçš„trainæ–¹æ³•å†…éƒ¨ åˆ†åˆ«è¿›è¡Œencodingå’Œpaddingã€‚ è‡³äºä¸ºä»€ä¹ˆè¿™æ ·åšï¼Œä»ä»£ç ä¸­æ²¡æœ‰çœ‹å‡ºåŸå› ã€‚å¯èƒ½æ˜¯ä¸ºäº†æ›´ç»†ç²’åº¦çš„æ§åˆ¶è®­ç»ƒå’Œè¯„ä¼°å„è‡ªåœºæ™¯ä¸‹çš„é€»è¾‘ã€‚ trl.trainer.utils.selective_log_softmax å…¶ä¸­å®ç°é’ˆå¯¹FP32 FP64 é‡‡ç”¨äº†é«˜æ•ˆçš„ logsumexpæ–¹æ³•ï¼Œé’ˆå¯¹å…¶ä»–çš„æ ¼å¼é‡‡ç”¨äº†ä½æ•ˆçš„æ–¹æ³•ï¼Œå› ä¸ºä½ç²¾åº¦æ ¼å¼è®¡ç®—å®¹æ˜“å‡ºç°é—®é¢˜ã€‚ logsumexp çš„æ ‡å‡†å®šä¹‰æ˜¯ï¼š $$\\log \\sum_{i} \\exp(x_i)$$ï¼Œå¦‚æœç›´æ¥æŒ‰è¿™ä¸ªå…¬å¼å®ç°ï¼Œå½“ $x_i$ çš„å€¼å¾ˆå¤§æ—¶ï¼Œ$\\exp(x_i)$ å¯èƒ½ä¼šå¯¼è‡´æ•°å€¼ä¸Šæº¢ï¼ˆoverflowï¼‰ï¼Œè¶…å‡ºæµ®ç‚¹æ•°çš„è¡¨ç¤ºèŒƒå›´ï¼Œç»“æœå˜ä¸ºæ— ç©·å¤§ã€‚ å·¥ç¨‹ä¸Šçš„ç¨³å®šå®ç°æ–¹æ³• 1. ä¸ºäº†é¿å…ä¸Šæº¢ï¼Œlogsumexp åœ¨å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚ PyTorchã€TensorFlowï¼‰ä¸­éƒ½æœ‰ä¸€ä¸ªç»è¿‡ä¼˜åŒ–çš„ã€æ•°å€¼ç¨³å®šçš„å®ç°ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å¯¹æ•°å‡½æ•°çš„æ€§è´¨ï¼Œå°†æŒ‡æ•°è¿ç®—ä¸­çš„å¤§æ•°ç›¸åŠ é—®é¢˜ï¼Œè½¬æ¢ä¸ºå¯¹æ•°è¿ç®—ä¸­çš„å°èŒƒå›´æ•°ç›¸åŠ é—®é¢˜ã€‚ 2. å…·ä½“æ–¹æ³•å¦‚ä¸‹ï¼š æ‰¾åˆ°æœ€å¤§å€¼ï¼šé¦–å…ˆï¼Œæ‰¾åˆ°è¾“å…¥å‘é‡ $x$ ä¸­çš„æœ€å¤§å€¼ $x_{max}$ã€‚ è½¬æ¢å…¬å¼ï¼šåˆ©ç”¨ $e^{a+b} = e^a e^b$ çš„æ€§è´¨ï¼Œå°†åŸå…¬å¼è¿›è¡Œç­‰ä»·è½¬æ¢ï¼š $$\\log \\sum_{i} \\exp(x_i) = \\log \\left( \\exp(x_{max}) \\sum_{i} \\exp(x_i - x_{max}) \\right)$$ æ‹†åˆ†å¯¹æ•°ï¼šåˆ©ç”¨ $\\log(ab) = \\log(a) + \\log(b)$ çš„æ€§è´¨ï¼Œè¿›ä¸€æ­¥æ‹†åˆ†ï¼š $$= \\log(\\exp(x_{max})) + \\log \\left( \\sum_{i} \\exp(x_i - x_{max}) \\right)$$ $$= x_{max} + \\log \\left( \\sum_{i} \\exp(x_i - x_{max}) \\right)$$ ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ³•æ›´ç¨³å®šï¼Ÿ é˜²æ­¢ä¸Šæº¢ï¼šåœ¨è½¬æ¢åçš„å…¬å¼ä¸­ï¼Œ$x_i - x_{max}$ çš„å€¼éƒ½æ˜¯è´Ÿæ•°æˆ–è€…é›¶ã€‚è¿™æ„å‘³ç€ $\\exp(x_i - x_{max})$ çš„å€¼éƒ½åœ¨ $(0, 1]$ èŒƒå›´å†…ã€‚è¿™æ ·ï¼Œå³ä½¿ $x_i$ éå¸¸å¤§ï¼ŒæŒ‡æ•°è¿ç®—çš„ç»“æœä¹Ÿä¸ä¼šä¸Šæº¢ã€‚ ä¿æŒç²¾åº¦ï¼šè™½ç„¶ $x_i - x_{max}$ çš„å€¼æ˜¯è´Ÿæ•°ï¼Œä½†å®ƒä»¬ä¹‹é—´çš„ç›¸å¯¹å¤§å°å…³ç³»ä¿æŒä¸å˜ï¼Œè¿™ä¿è¯äº†è®¡ç®—ç»“æœçš„ç²¾ç¡®åº¦ã€‚ è®¡ç®—æ•ˆç‡ï¼šè¿™ä¸ªç¨³å®šçš„å®ç°åªéœ€è¦é¢å¤–è¿›è¡Œä¸€æ¬¡ max è¿ç®—å’Œä¸€æ¬¡åŠ æ³•è¿ç®—ï¼Œå¯¹æ•´ä½“è®¡ç®—æ•ˆç‡å½±å“å¾ˆå°ã€‚ ä¾‹å¦‚ï¼Œåœ¨ PyTorch ä¸­ï¼Œtorch.logsumexp å‡½æ•°å°±æ˜¯ä»¥è¿™ç§æ–¹å¼å®ç°çš„ã€‚å½“ä½ ä½¿ç”¨å®ƒæ—¶ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨å¤„ç†è¿™äº›æ•°å€¼ç¨³å®šæ€§çš„ç»†èŠ‚ã€‚ è¿™ä¸ªå·¥ç¨‹ä¼˜åŒ–æ–¹æ³•åœ¨æ·±åº¦å­¦ä¹ ä¸­éå¸¸é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¯¸å¦‚ Softmax äº¤å‰ç†µæŸå¤±ã€ä¿¡å¿µä¼ æ’­ï¼ˆBelief Propagationï¼‰ç­‰éœ€è¦å¤§é‡æŒ‡æ•°å’Œå¯¹æ•°è¿ç®—çš„åœºæ™¯ã€‚ PPOç­–ç•¥çš„æµç¨‹(from chatgpt) ^PPO-process ä¸€ã€PPOç®—æ³•æ ¸å¿ƒæ€æƒ³ PPO å±äº**ç­–ç•¥æ¢¯åº¦ï¼ˆPolicy Gradientï¼‰**å®¶æ—ï¼Œç›®æ ‡æ˜¯é€šè¿‡ä¸æ–­ä¼˜åŒ–ç­–ç•¥å‚æ•°ï¼Œä½¿å¾—æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è·å¾—æ›´é«˜çš„æœŸæœ›å›æŠ¥ã€‚ å®ƒçš„å…³é”®åœ¨äºï¼šåœ¨æ›´æ–°ç­–ç•¥æ—¶é™åˆ¶æ›´æ–°å¹…åº¦ï¼Œé˜²æ­¢ç­–ç•¥æ”¹å˜å¤ªå¤§å¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚ äºŒã€PPOçš„åŸºæœ¬æµç¨‹ï¼ˆå…¸å‹ç‰ˆæœ¬ï¼šPPO-Clipï¼‰ é‡‡æ ·ï¼ˆRolloutï¼‰ ä½¿ç”¨å½“å‰ç­–ç•¥ $\\pi_{\\theta_{old}}(a_t|s_t)$ï¼Œä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ä¸€æ‰¹æ•°æ®ï¼š $(s_t, a_t, r_t, s_{t+1})$ å¹¶è®¡ç®—æŠ˜æ‰£å›æŠ¥ $R_t$ ã€‚ è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantage Estimateï¼‰ é€šå¸¸ç”¨ GAEï¼ˆGeneralized Advantage Estimationï¼‰ï¼š $\\hat{A}t = \\sum{l=0}^{\\infty} (\\gamma\\lambda)^l \\delta_{t+l}$ å…¶ä¸­ $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ã€‚ è®¡ç®—é‡è¦æ€§æ¯”ç‡ï¼ˆratioï¼‰ $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ è¡¨ç¤ºæ–°æ—§ç­–ç•¥åœ¨åŒä¸€åŠ¨ä½œä¸Šçš„â€œæ¦‚ç‡å˜åŒ–â€ã€‚ æ„å»ºPPOçš„ç›®æ ‡å‡½æ•°ï¼ˆClipped Surrogate Objectiveï¼‰ $\\large L^{CLIP}(\\theta) = \\mathbb{E}_t \\big[ \\min( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t )\\big]$ å½“ $r_t(\\theta)$ åç¦» 1 å¤ªå¤šï¼ˆè¶…è¿‡ Â±Îµï¼‰æ—¶ï¼Œä¼šè¢«æˆªæ–­ï¼ˆclipï¼‰ï¼Œé˜²æ­¢è¿‡åº¦æ›´æ–°ã€‚ Îµ é€šå¸¸å– 0.1ï½0.2ã€‚ ä¼˜åŒ–ç›®æ ‡ + å€¼å‡½æ•° + ç†µæ­£åˆ™ å®é™…ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°é€šå¸¸æ˜¯ä¸‰é¡¹ä¹‹å’Œï¼š $L(\\theta) = L^{CLIP}(\\theta) - c_1 L^{VF}(\\theta) + c_2 S[\\pi_\\theta]$ $L^{VF}$ï¼šå€¼å‡½æ•°çš„MSEæŸå¤± $S[\\pi_\\theta]$ï¼šç­–ç•¥çš„ç†µï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰ å¤šæ¬¡å°æ­¥æ›´æ–°ï¼ˆK epochsï¼‰ ç”¨åŒä¸€æ‰¹é‡‡æ ·æ•°æ®ï¼Œåœ¨æ¯ä¸ª mini-batch ä¸Šä¼˜åŒ– K è½®ã€‚ æ›´æ–°æ—§ç­–ç•¥å‚æ•° $\\theta_{old} \\leftarrow \\theta$ PPOTrainerä¸­ä»£ç çš„æµç¨‹æ€»ç»“ ^PPO-code-process\nå¤–å¾ªç¯ï¼šæ€»è®­ç»ƒbatchæ•° å¯¹ policy model ç”Ÿæˆçš„æ¯ä¸€ä¸ªåºåˆ—æ ·æœ¬ï¼Œè¾“å…¥åˆ°reward model ï¼Œåºåˆ—ä¸­æœ€åä¸€ä¸ªtokençš„ hidden state è¢«è¾“å…¥åˆ°ä¸€ä¸ªçº¿æ€§å¤´ï¼Œæœ€ç»ˆè¾“å‡ºä¸€ä¸ªæ ‡é‡å€¼ï¼Œè¡¨ç¤ºåºåˆ—äºŒåˆ†ç±»çš„æ‰“åˆ†scoreã€‚ å¯¹ policy model ç”Ÿæˆçš„æ¯ä¸€ä¸ªåºåˆ—æ ·æœ¬ï¼Œæ¯ä¸€ä¸ªtokenè¾“å‡ºå‰çš„vocabulary åˆ†å¸ƒå’Œ ref_modelçš„åˆ†å¸ƒ ä¸¤è€…è®¡ç®—klæ•£åº¦ï¼ˆä½¿ç”¨äº†è¿‘ä¼¼æ–¹æ³•ä½¿å¾—è®¡ç®—åŠ é€Ÿï¼‰ï¼Œå³è¾“å‡ºä¸€ä¸ª klæ•£åº¦åºåˆ—ï¼Œåºåˆ—ä¸­æ¯ä¸ªå…ƒç´ è¡¨ç¤ºå¯¹åº”ä½ç½®tokençš„klæ•£åº¦ã€‚ å°†åºåˆ—çº§çš„score åŠ åˆ°klåºåˆ—ä¸­æ¯ä¸€ä¸ªä½ç½®ï¼Œç»“æœå³ä¸º rewards åºåˆ— ä¾æ¬¡è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ gaeï¼ŒGAE(t)=rewards(t) + gamma*value(t+1) - value(t) + gamma*lam*GAE(t+1)ï¼Œè¿™é‡Œlamæ˜¯æŒ‡lambda è®¡ç®— return return(t) = GAE(t)+ value(t) ã€‚ï¼ˆè¿™é‡ŒæŠŠvalueç†è§£æˆæ˜¯ tæ­¥çš„çŠ¶æ€stateä¸‹ï¼Œæœªæ¥å¥–åŠ±çš„æ€»å’ŒæœŸæœ›ï¼Œå³ çŠ¶æ€æœ¬èº«çš„ä»·å€¼ï¼‰ã€‚ valueå³ state value functionï¼ŒGAEå³ é’ˆå¯¹çš„æ˜¯actionã€‚ å†…å¾ªç¯ï¼šé’ˆå¯¹ä¸Šè¿°æ­¥éª¤ç”Ÿæˆçš„åºåˆ—æ ·æœ¬å’Œå¯¹åº”çš„returnã€GAEã€æ¦‚ç‡logp åºåˆ—ï¼Œå°†æ ·æœ¬æ‹†åˆ†æˆ micro batchï¼Œæ‰§è¡Œä¸‹åˆ—æ­¥éª¤ value ä¼˜åŒ–çš„æŸå¤±å‡½æ•°ä¸º (return - value(t))^2ï¼Œå…¶å®å°±æ˜¯çº¦æŸ ä¼˜åŠ¿advantage GAE æœ¬èº«å°½é‡ã€‚ ä½¿ç”¨å½“å‰çš„policy model è®¡ç®—action çš„æ–°æ¦‚ç‡å€¼ $\\large \\log p_{\\theta}$ policy gradientæŸå¤±ä¸º $\\Large - GAE * exp(\\log p_{\\theta}-\\log p_{\\theta_{old}})$ æˆ‘æƒ³è¿™é‡Œè¿™ä¹ˆå†™æ˜¯ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œå› ä¸ºä¸€èˆ¬æ˜¯è®©æ¨¡å‹è¾“å‡º logpï¼Œç„¶åè®¡ç®— æ¦‚ç‡æ¯”ï¼Œé‚£ä¹ˆå°±ç›´æ¥ç›¸å‡ç„¶åå–æŒ‡æ•°ã€‚èƒŒåå…¶å®å°±æ˜¯ logsumexpç®—å­ã€‚ ä¸¤ä¸ªæŸå¤±åŠ è½½ä¸€èµ·è¿›è¡Œåå‘ä¼ æ’­ï¼Œå³ä¼šä¿®æ”¹å½“å‰çš„policy_modelå’Œvalue_model è¾“å‡ºå’Œä¿å­˜å„ç§æ¨¡å‹ local_rollout_forward_batch_size æ˜¯æ¯ä¸ªèŠ‚ç‚¹ï¼Œåœ¨æœ¬åœ°è¿›è¡Œå¤šæ‰¹æ¬¡çš„è®­ç»ƒï¼Œæ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°å³ä¸º local_rollout_forward_batch_sizeã€‚\npolicy_modelå’Œref_modelåœ¨rolloutä¸­è°ƒç”¨çš„æ–¹æ³•ä¸åŒ 3. policy_modelä½¿ç”¨trl.trainer.utils.batch_generationï¼ˆæ˜¯å¯¹æ‰€æœ‰è®¡ç®—èŠ‚ç‚¹çš„å¹¶è¡Œæ‰¹é‡åŒ–è®¡ç®—ï¼‰ã€‚å°±æ˜¯åœ¨queryä¹‹åæ‹¼æ¥é¢„æµ‹å‡ºæ¥çš„responseã€‚ 1. batch_generation ä½¿ç”¨generation_configå‚æ•°ï¼Œä»£ç ä¸­è§„å®š max_new_tokens=args.response_lengthï¼Œå³ç¡¬æ€§æŒ‡å®šäº†responseé•¿åº¦ã€‚ 2. responseåœ¨ç”Ÿæˆå‡ºæ¥ä¹‹åï¼Œä¼šåˆå¹¶æ‰€æœ‰çš„ç«‹é©¬è¿›è¡Œå³paddingã€‚ 3. å…¶ä¸­è°ƒç”¨äº†GenerationMixinçš„generateæ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„è¾“å‡ºä¸ºtoken idsçš„åºåˆ—ã€‚ 4. ref_model ä½¿ç”¨ forwardã€‚å°±æ˜¯å°†æ•´ä¸ªquery_responseæ‹¼æ¥ç»“æœå…¨éƒ¨è¾“å…¥åˆ°ref_modelï¼Œä¸€æ¬¡æ€§å¾—å‡ºé”™ä½çš„é¢„æµ‹ç»“æœã€‚æ‰€ä»¥å¯¹logitsç´¢å¼•çš„æ—¶å€™ä¼šå¾€å‰é”™ä¸€ä½ï¼Œå¹¶ä¸”æœ€åä¸€ä¸ªä½ç½®æ˜¯ä¸éœ€è¦ä½¿ç”¨çš„ã€‚ 1. æˆ‘æƒ³batch_generationæ˜¯éœ€è¦è¿›è¡Œå¤æ‚paddingçš„ï¼Œä»è€Œå¯èƒ½å¯¼è‡´æ¯ä¸€ä¸ªå°batchç”Ÿæˆçš„è¾“å‡ºé•¿åº¦æ˜¯ä¸ä¸€è‡´çš„ã€‚ä½†æˆ‘æƒ³ä¸åˆ°ä¸ºä»€ä¹ˆè¿™ä¹ˆè°ƒç”¨çš„ç†ç”±ã€‚ 5. æ€»ä½“ä¸ŠPPOæœ‰ä¸¤ä¸ªåœ°æ–¹ç‰µæ¶‰åˆ°æ¦‚ç‡çš„å¯¹æ¯” 1. ç¬¬ä¸€ä¸ªæ˜¯ policy_modelå’Œref_modelä¸¤ä¸ªæ¨¡å‹çš„KLæ•£åº¦ï¼Œæ”¾ç½®policy_modelè·‘å¤ªè¿œè¿‡äºç¦»è°±ã€‚ 2. ç¬¬äºŒä¸ªæ˜¯ policy_modelå’Œold_policy_modelçš„æ ·æœ¬æ¦‚ç‡å¯¹æ¯”ï¼Œä½†ä¸éœ€è¦ä¿ç•™old_policy_oldè¿™ä¸ªæ¨¡å‹ï¼Œå› ä¸ºè®­ç»ƒä¸­ä»…ä»…æ˜¯ä½¿ç”¨äº† old_policy_modelçš„æ ·æœ¬å’Œå…¶æ ·æœ¬æ¦‚ç‡ï¼Œæ‰€ä»¥é€šè¿‡old_policy_modelä¸€æ¬¡é‡‡æ ·å‡ºæ¥ä¸€å¤§å †æ•°æ®ä¹‹åï¼ˆå…¶ä¸­ä¿ç•™äº†æ ·æœ¬æ¦‚ç‡ $\\Large p_{theta_{old}}$ï¼‰ï¼Œ old_policy_modelå°±å¯ä»¥ä¸¢å¼ƒï¼Œä»…ä»…é€šè¿‡ä½¿ç”¨é‡‡æ ·çš„æ ·æœ¬å’Œæ ·æœ¬æ¦‚ç‡æ¥è¿›è¡Œpolicy_modelçš„è®­ç»ƒä¼˜åŒ–ã€‚ é‚£ä¹ˆå°±ç›¸å½“äºæ˜¯ä¸¤ä¸ªæ¨¡å‹åˆäºŒä¸ºä¸€ã€‚\nclass Qwen2ForSequenceClassification(Qwen2PreTrainedModel): The Qwen2 Model transformer with a sequence classification head on top (linear layer).\n[Qwen2ForSequenceClassification] uses the last token in order to do the classification, as other causal models (e.g. GPT-2) do. è®¡ç®—çš„æ—¶å€™ä¼šæ‰¾åˆ° responseä¸­çš„ last_non_pad_tokenï¼Œè¾“å‡ºå¯¹åº”çš„logitisï¼Œç„¶åç»è¿‡scoreæ–¹æ³• è¿›è¡Œlinear_unit è®¡ç®—ã€‚\nclass AlbertForSequenceClassification(AlbertPreTrainedModel): Albert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.\ndataclassÂ æ³¨è§£æ˜¯ Python 3.7 åŠä»¥ä¸Šå¼•å…¥çš„ä¸€ä¸ªè£…é¥°å™¨ï¼Œä½œç”¨æ˜¯ç®€åŒ–ç±»çš„ç¼–å†™ï¼Œè®©ç±»è‡ªåŠ¨è·å¾—ä¸€äº›å¸¸ç”¨æ–¹æ³•ï¼ˆå¦‚Â __init__,Â __repr__,Â __eq__Â ç­‰ï¼‰ï¼Œç”¨äºè¡¨ç¤ºæ•°æ®ç»“æ„ã€‚\nä¸ºä»€ä¹ˆ è®¡ç®—ref_logprob ä½¿ç”¨ selective_log_softmaxæ–¹æ³•ï¼ŒæŒ‰ç†è¯´KLæ•£åº¦åº”è¯¥æ˜¯å¯¹vocabularyçš„æ‰€æœ‰è¯è¿›è¡ŒKLæ•£åº¦è®¡ç®—å•Šï¼Ÿ ç†è®ºä¸Šï¼ŒKLæ•£åº¦çš„å®šä¹‰æ˜¯ï¼š $$K L \\left(\\right. p \\parallel q \\left.\\right) = \\underset{i}{\\sum} p \\left(\\right. i \\left.\\right) log â¡ \\frac{p \\left(\\right. i \\left.\\right)}{q \\left(\\right. i \\left.\\right)}$$ è¿™é‡Œ (i) æ˜¯æ•´ä¸ª vocabulary çš„æ‰€æœ‰ tokenã€‚ å®é™…å·¥ç¨‹å®ç°ï¼ˆRLHF/PPOåœºæ™¯ï¼‰ï¼š 6. æˆ‘ä»¬é€šå¸¸åªå…³å¿ƒæ¨¡å‹å®é™…â€œèµ°å‡ºçš„è·¯å¾„â€ï¼Œå³ç”Ÿæˆçš„ token åºåˆ—ã€‚ 7. PPO/Reward Modeling é‡Œï¼ŒKLé¡¹æ˜¯ç”¨æ¥çº¦æŸæ–°æ¨¡å‹ï¼ˆpolicyï¼‰ä¸è¦åç¦»æ—§æ¨¡å‹ï¼ˆreference/policyï¼‰çš„è¡Œä¸ºï¼Œåªéœ€è¦å¯¹â€œå·²é‡‡æ ·çš„ tokenâ€ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒåšçº¦æŸã€‚\nrewards[[actual_start, actual_end]] += scores èµ‹å€¼æ“ä½œï¼Œrewardsä¸­å­˜æ”¾çš„æ˜¯klæ•£åº¦ï¼Œè¿™é‡Œåˆ™æ˜¯å°†klæ•£åº¦å¯¹åº”æœ€åä¸€ä¸ªtokenä½ç½®å‘å³é”™ä¸€ä½çš„ä½ç½®åŠ ä¸Šä¸€ä¸ªæœ€ç»ˆçš„reward scoreã€‚ è¿™é‡Œçš„actual_startæ˜¯æ ·æœ¬çš„indexï¼Œactual_endæ˜¯æŒ‡æ¯ä¸€ä¸ªæ ·æœ¬çš„æœ€åä¸€ä¸ªtokenä½ç½®+1ã€‚\nåœ¨ Hugging Face Transformers ä¸­ï¼Œç±»åå‰çš„Â AutoÂ è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©æ¨¡å‹çš„æ„æ€ã€‚ 8. AutoModelForSequenceClassificationÂ ä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„æ¨¡å‹ç±»ï¼Œè€Œæ˜¯ä¸€ä¸ªå·¥å‚ç±»ã€‚ 9. å®ƒå¯ä»¥æ ¹æ®ä½ åŠ è½½çš„ checkpointï¼ˆå¦‚Â \"bert-base-uncased\"ã€\"roberta-base\"ã€è‡ªå®šä¹‰è·¯å¾„ç­‰ï¼‰ï¼Œè‡ªåŠ¨å®ä¾‹åŒ–å¯¹åº”çš„å…·ä½“æ¨¡å‹ç±»ï¼ˆå¦‚Â BertForSequenceClassificationã€RobertaForSequenceClassificationÂ ç­‰ï¼‰ã€‚ 10. è¿™è®©ä½ ä¸éœ€è¦å…³å¿ƒåº•å±‚æ˜¯å“ªä¸ªæ¨¡å‹ï¼Œåªè¦ä¼ å…¥æ¨¡å‹åæˆ–è·¯å¾„ï¼Œå®ƒä¼šè‡ªåŠ¨å¸®ä½ é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹å®ç°ã€‚\nPPOä¸­ value_modelä¸€èˆ¬ä½¿ç”¨çš„æ˜¯ sequence_classification modelï¼Œæ¯”å¦‚ ç¤ºä¾‹è„šæœ¬ ä¸­ä»£ç å¦‚ä¸‹ï¼Œvalue_modelå’Œreward_modelä½¿ç”¨çš„æ˜¯åŒä¸€ä¸ªæ¨¡å‹ç±»çš„ä¸åŒå®ä¾‹ã€‚\nvalue_model = AutoModelForSequenceClassification.from_pretrained( training_args.reward_model_path, trust_remote_code=model_args.trust_remote_code, num_labels=1 ) reward_model = AutoModelForSequenceClassification.from_pretrained( training_args.reward_model_path, trust_remote_code=model_args.trust_remote_code, num_labels=1 ) è¿™é‡Œä¸¾ä¾‹ transformers.models.qwen2.modeling_qwen2.Qwen2ForSequenceClassification\nPPOTrainerä¸­è°ƒç”¨get_rewardçš„æ—¶å€™ï¼Œä½¿ç”¨çš„æ˜¯ sequence_classfication_modelçš„ base_model_prefixæŒ‡å‘çš„åº•å±‚LLMåŸå§‹æ¨¡å‹ ã€ scoreæ–¹æ³•ï¼ˆscoreæ–¹æ³•å°±æ˜¯åœ¨baseLLMæ¨¡å‹åæ·»åŠ ä¸€ä¸ªçº¿æ€§å±‚ï¼Œä»¥æ˜ å°„åˆ°logitsï¼‰ã€‚å…¶æ•ˆæœå°±æ˜¯å¯¹ä¸€å¯¹é—®ç­”å­—ç¬¦ä¸²åºåˆ—è¾“å‡ºä¸€ä¸²rewardæ•°å€¼ã€‚\nget_rewardæ–¹æ³•ä¸­çš„å…·ä½“é€»è¾‘ã€‚ lm_backboneåº•å±‚ä½¿ç”¨çš„æ˜¯åŸºæ¨¡å‹ï¼Œå…¶ä¸­ output_hidden_states è¡¨ç¤ºè¦æŠŠæ¨¡å‹ä¸­æ‰€æœ‰å±‚çš„hidden_stateså…¨éƒ¨è¿›è¡Œè¾“å‡ºã€‚æ‰€ä»¥åœ¨è°ƒç”¨scoreçš„æ—¶å€™ä¼šåªå–æœ€åä¸€å±‚çš„hidden_statesï¼Œç„¶åè¾“å…¥åˆ°scoreæ–¹æ³•ä¸­ï¼ˆå³å†ç»è¿‡ä¸€å±‚çº¿æ€§å±‚å¾—åˆ°logitsï¼‰ã€‚æ•ˆæœå°±æ˜¯å¯¹äºé—®ç­”å­—ç¬¦ä¸²åºåˆ—ä¸­çš„æ¯ä¸€ä¸ªtokenéƒ½ä¼šå¾—åˆ°ä¸€ä¸ªreward_logitsæ•°å€¼ã€‚\noutput = lm_backbone( input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, return_dict=True, output_hidden_states=True, use_cache=False, # otherwise mistral-based RM would error out ) reward_logits = model.score(output.hidden_states[-1]) # è¿™é‡Œçš„output.hidden_states è¡¨ç¤ºbase_modelçš„æ‰€æœ‰å±‚çš„è¾“å‡º # output.hidden_states[-1] åˆ™è¡¨ç¤ºæœ€åä¸€å±‚çš„è¾“å‡º # Qwen2ForSequenceClassification çš„forwardä¸­è®¡ç®— lossï¼Œå°±æ˜¯æŒ‰ç…§æœ€åä¸€ä¸ªtokençš„æœ€åä¸€å±‚è¾“å‡º + çœŸå®label ä¸€èµ·è®¡ç®—å‡ºäº¤å‰ç†µ return ( reward_logits, reward_logits[ torch.arange(reward_logits.size(0), device=reward_logits.device), sequence_lengths, ].squeeze(-1), sequence_lengths, ) # è¿™é‡Œè¡¨ç¤ºè¿”å› responseåºåˆ—ä¸­æœ€åä¸€ä¸ªåˆæ³•tokençš„logitsè¾“å‡º transformers.models.qwen2.modeling_qwen2.Qwen2ForSequenceClassification æ˜¯ä¸€ä¸ªä¾‹å­æ¨¡å‹ï¼Œè¯¥æ¨¡å‹çš„forwardæ–¹æ³•ä¸­è®¡ç®—äº†æ¯ä¸€ä¸ªtokenè¾“å‡ºçš„åˆ†ç±»çš„logitsï¼Œç„¶åä»…ä»…è·å–äº†æ¯ä¸€ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ª non_padding_tokençš„logitsä½œä¸ºè¾“å‡ºã€‚\nreward_modelå¾—åˆ°çš„ä¸€ä¸ªåºåˆ—ä¸€ä¸ªrewardå€¼ï¼Œä½†å…¶å®æ˜¯æœ€åä¸€æ­¥çš„immediate rewardã€‚ value_modelå¾—åˆ°çš„æ¯ä¸€ä¸ªåŠ¨ä½œï¼ˆå³tokenï¼‰ä¸€ä¸ªvalueå€¼ï¼ˆå³é•¿æœŸæ•ˆæœçš„è¯„ä¼°çš„æŒ‡æ ‡ï¼‰ã€‚\nPPOTrainerä¸­ ä¸ºä»€ä¹ˆéœ€è¦æœ‰ missing_eos_penaltyï¼Ÿ\nåœ¨Â PPOTrainerÂ ä¸­å­˜åœ¨Â missing_eos_penaltyï¼Œä¸»è¦æ˜¯ä¸ºäº†è§£å†³Â ç”Ÿæˆç»“æœæ²¡æœ‰åŒ…å«ç»ˆæ­¢ç¬¦ï¼ˆå¦‚Â eos_token_idï¼‰çš„æƒ…å†µï¼Œé˜²æ­¢æ¨¡å‹ç”Ÿæˆä¸å®Œæ•´æˆ–å¼‚å¸¸çš„å“åº”ã€‚\nåœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ¨¡å‹é€šå¸¸ä¼šåœ¨å“åº”ç»“å°¾ç”Ÿæˆä¸€ä¸ªâ€œç»ˆæ­¢ç¬¦â€ï¼ˆä¾‹å¦‚Â eos_token_idï¼‰ï¼Œè¡¨ç¤ºå“åº”ç»“æŸã€‚ å¦‚æœæ¨¡å‹æ²¡æœ‰ç”Ÿæˆç»ˆæ­¢ç¬¦ï¼Œå“åº”å¯èƒ½ï¼š è¶…é•¿ï¼ˆä¸€ç›´ç”Ÿæˆåˆ°æœ€å¤§é•¿åº¦ï¼‰ ä¸å®Œæ•´ï¼ˆç¼ºå°‘è¯­æ³•ä¸Šçš„ç»“å°¾ï¼‰ å½±å“åç»­è¯„ä¼°å’Œè®­ç»ƒï¼ˆå¦‚å¥–åŠ±æ¨¡å‹ã€PPOç­‰ï¼‰ å¦‚æœå“åº”æ²¡æœ‰ç»ˆæ­¢ç¬¦ï¼Œå¯èƒ½æ˜¯æ¨¡å‹æ²¡å­¦ä¼šâ€œä½•æ—¶ç»“æŸâ€ï¼Œè¿™ç§å“åº”ä¸€èˆ¬æ˜¯ä¸ç¬¦åˆä»»åŠ¡é¢„æœŸçš„ï¼Œéœ€è¦æƒ©ç½šã€‚ missing_eos_penaltyÂ å°±æ˜¯å¯¹è¿™ç§æƒ…å†µåŠ ä¸€ä¸ªè´Ÿåˆ†ï¼Œé¼“åŠ±æ¨¡å‹åœ¨åˆé€‚çš„æ—¶å€™ç”Ÿæˆç»ˆæ­¢ç¬¦ã€‚ Approximating KL Divergence\nPPOTrainerä¸­ä¸ºä»€ä¹ˆè¦whiten_rewards åœ¨Â PPOTrainerÂ ä¸­ï¼Œå­˜åœ¨ä¸€ä¸ªå‚æ•°Â whiten_rewardsï¼Œå…¶ä½œç”¨æ˜¯å¯¹å¥–åŠ±ï¼ˆrewardï¼‰è¿›è¡Œå½’ä¸€åŒ–/æ ‡å‡†åŒ–ï¼ˆwhiteningï¼‰ã€‚ å¥–åŠ±çš„å°ºåº¦å’Œåˆ†å¸ƒç›´æ¥å½±å“ä¼˜åŠ¿çš„åˆ†å¸ƒï¼Œè€Œä¼˜åŠ¿åˆ†å¸ƒåˆå½±å“æ¢¯åº¦æ›´æ–°çš„ç¨³å®šæ€§å’Œè®­ç»ƒé€Ÿåº¦ã€‚ å¦‚æœå¥–åŠ±å¾ˆå¤§æˆ–å¾ˆå°ï¼Œä¼šå¯¼è‡´ç­–ç•¥æ¢¯åº¦å¾ˆå¤§/å¾ˆå°ï¼Œå½±å“æ”¶æ•›é€Ÿåº¦ï¼Œç”šè‡³å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±ã€‚\né‚£ä¹ˆåœ¨é‡‡æ ·é˜¶æ®µè®¡ç®—çš„rewardã€ valueç­‰æ•°æ®ï¼Œå› ä¸ºåœ¨è®­ç»ƒé˜¶æ®µè¿™äº›æ•°æ®ä¸­çš„å¾ˆå¤šä¼šé‡æ–°ç”Ÿæˆï¼Œæ‰€ä»¥é‡‡æ ·é˜¶æ®µå¾ˆå¤šè®¡ç®—æ˜¯ä¸æ˜¯æµªè´¹äº†?\né‡‡æ ·é˜¶æ®µçš„ value/logprobåªç”¨ä¸€æ¬¡ï¼Œè®­ç»ƒé˜¶æ®µä¼šé‡æ–°ç®—â€œæ–°ç­–ç•¥â€çš„ value/logprobã€‚ for t in reversed(range(gen_length)): nextvalues = values[:, t + 1] if t \u003c gen_length - 1 else 0.0 delta = rewards[:, t] + args.gamma * nextvalues - values[:, t] lastgaelam = delta + args.gamma * args.lam * lastgaelam advantages_reversed.append(lastgaelam) advantages = torch.stack(advantages_reversed[::-1], axis=1) returns = advantages + values advantages = masked_whiten(advantages, ~padding_mask) advantages = torch.masked_fill(advantages, padding_mask, 0) è¿™é‡Œè®¡ç®—returnå’Œadvantageã€‚returnå°±æ˜¯ state valueçš„ä¸€ä¸ªçœŸå®é‡‡æ ·ï¼Œç”¨æ¥è®¡ç®— value_lossï¼Œå³returnå’Œä¼°è®¡çš„valueä¹‹é—´çš„å¹³æ–¹æŸå¤±ã€‚advantage å°±æ˜¯ ä¼˜åŠ¿ï¼Œå³å½“å‰æ—¶åˆ»ä»¥åŠæ¯ä¸ªä¹‹åçš„æ—¶åˆ»æ‰€è®¡ç®—çš„advantageçš„æ‰“æŠ˜ä¹‹å’Œï¼Œç”¨æ¥ç»™ç­–ç•¥æ¢¯åº¦åŠ æƒã€‚\næŸå¤±å‡½æ•°\nlogprobs_diff = new_logprobs - mb_logprobs ratio = torch.exp(logprobs_diff) pg_losses = -mb_advantage * ratio pg_losses2 = -mb_advantage * torch.clamp(ratio, 1.0 - args.cliprange, 1.0 + args.cliprange) pg_loss_max = torch.max(pg_losses, pg_losses2) pg_loss = masked_mean(pg_loss_max, ~padding_mask[micro_batch_inds]) loss = pg_loss + args.vf_coef * vf_loss è¿™å—ä»£ç ä¸­ä¸ºä»€ä¹ˆæ²¡æœ‰ä½¿ç”¨ logpçš„å¯¼æ•°ï¼ŒåŸå› æ˜¯ $\\Large \\mathbb{E}{\\beta} \\left[ \\frac{\\pi{\\theta}(a|s)}{\\beta(a|s)} Q^{\\pi}(s,a)\\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s) \\right]$ ï¼Œå…¶å®å°±æ˜¯ $\\Large \\mathbb{E}{\\beta} \\left[ \\frac{\\nabla{\\theta} \\pi_{\\theta}(a|s)}{\\beta(a|s)} Q^{\\pi}(s,a) \\right]$ï¼Œé‚£ä¹ˆå¯¹åº”çš„æŸå¤±å‡½æ•°å°±æ˜¯ä»¥$\\large \\theta$ä¸ºä¼˜åŒ–å‚æ•°çš„ $\\Large - \\mathbb{E}{\\beta} \\left[ \\frac{\\pi{\\theta}(a|s)}{\\beta(a|s)} Q^{\\pi}(s,a) \\right]$ã€‚\nmb_logprobs åœ¨lossä¸­ä¸ä¼šåå‘ä¼ æ’­æ¢¯åº¦ä¹ˆ å› ä¸º mb_logprobsè®¡ç®—çš„æ—¶å€™æ˜¯åœ¨ with torch.no_grad(): ä¸­ã€‚\nvf_losses1 = torch.square(vpred - mb_return) è¿™é‡Œçš„mb_return å°±æ˜¯åç»­å¤šæ­¥ç´¯ç§¯çš„rewardå€¼ã€‚\nä¸ºä»€ä¹ˆpolicy loss å’Œ value function loss è¦åŠ èµ·æ¥è¿›è¡Œoptimizeï¼Ÿ\nå¦‚æœä½ è®¾è®¡äº†åˆ†ç¦»çš„ policy model å’Œ value modelï¼ˆå³ Actor å’Œ Critic å®Œå…¨åˆ†ç¦»ï¼‰ï¼Œé‚£ä¹ˆç¡®å®å¯ä»¥åˆ†å¼€ optimizeã€åˆ†å¼€ backwardã€‚ ä½†æœ€ä¸»æµçš„å®ç°ï¼ˆæ¯”å¦‚ Huggingface Transformers çš„ PPOTrainerï¼‰æ˜¯åˆä¸€æ¨¡å‹ï¼Œä¸€ä¸ªæ¨¡å‹é‡Œæœ‰ä¸¤ä¸ªè¾“å‡º headï¼Œå‚æ•°æ˜¯å…±äº«çš„ï¼Œæ‰€ä»¥å¿…é¡»æŠŠ loss åˆåœ¨ä¸€èµ·ï¼Œç»Ÿä¸€ backward å’Œ optimizeã€‚ ä¼˜åŒ–å™¨ï¼ˆoptimizerï¼‰ä¼šå¯¹æ‰€æœ‰å‚æ•°åšæ¢¯åº¦æ›´æ–°ï¼Œpolicy head å’Œ value headçš„æ¢¯åº¦ä¼šåˆ†åˆ«å›ä¼ åˆ°ä¸»å¹²å’Œå„è‡ª headã€‚ åœ¨train æ–¹æ³•å†…ï¼Œref_policy æ˜¯è‡ªå§‹è‡³ç»ˆ ä¸€è‡´ä¿æŒä¸å˜çš„ã€‚ref_policy åªæ˜¯é¿å… policy_model è·‘å¾—å¤ªè¿œã€‚è€Œæ ·æœ¬é‡ç”¨æ˜¯ä½¿ç”¨importantce weightæ¥è§£å†³çš„ï¼Œå³ $\\large \\mathbb{E}{\\beta} \\left[ \\frac{ \\pi{\\theta}(a|s)}{\\beta(a|s)} \\right]$ ã€‚\nè¿™ä¸ªPPOç®—æ³•çš„å®ç°ï¼Œä¸ºä»€ä¹ˆæ—¢æœ‰klæ•£åº¦è®¡ç®—ä½œä¸º rewardï¼ŒåŒæ—¶åˆæœ‰clipæ“ä½œ\nclip objectiveï¼šæ§åˆ¶å•æ¬¡æ›´æ–°ä¸è¦å¤ªå¤§ï¼Œé˜²æ­¢è®­ç»ƒä¸ç¨³å®šã€‚ KL æ•£åº¦å¥–åŠ±ï¼šä¿è¯æ•´ä½“ç­–ç•¥ä¸ä¼šé€æ¸åç¦»å‚è€ƒæ¨¡å‹ï¼ˆhuman-preference-aligned policyï¼‰å¤ªè¿œã€‚ KL æ•£åº¦ç›¸å½“äºåœ¨ å¥–åŠ±å±‚é¢æƒ©ç½šç­–ç•¥è¿œç¦»å‚è€ƒæ¨¡å‹ï¼Œé¿å…æ¨¡å‹è·‘åï¼›è¿™å°±æ˜¯æ‰€è°“çš„â€œKL å¥–åŠ±å¡‘å½¢â€ã€‚ clip è§£å†³ çŸ­æœŸè®­ç»ƒç¨³å®šæ€§ï¼› KL å¥–åŠ±è§£å†³ é•¿æœŸåç§»é—®é¢˜ï¼Œç›¸å½“äºç»™æ¨¡å‹åŠ äº†ä¸ªâ€œç‰µå¼•ç»³â€ã€‚ ä½ çœ‹åˆ°çš„å®ç°é‡Œï¼ŒKL æ•£åº¦ä¸æ˜¯ç›´æ¥å½“çº¦æŸç”¨ï¼Œè€Œæ˜¯ä½œä¸ºå¥–åŠ±é¡¹å‚ä¸å›æŠ¥è®¡ç®—ï¼›è€Œ clip åˆ™åœ¨ä¼˜åŒ–ç›®æ ‡é‡Œçº¦æŸç­–ç•¥æ›´æ–°ã€‚è¿™ä¸¤è€…æ˜¯äº’è¡¥å…³ç³»ã€‚\nDPO TRPOä¸­çš„POå’Œ DPOä¸­çš„POæŒ‡çš„ä¸æ˜¯ä¸€ä¸ªä¸œè¥¿ï¼Ÿ\nTRPOï¼šå®ƒçš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç¯å¢ƒå¥–åŠ±ã€‚TRPO (Trust Region Policy Optimization) DPOï¼šå®ƒçš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–äººç±»åå¥½ã€‚ DPO (Direct Preference Optimization) DPOçš„ç›´è§‰åŒ–ç†è§£\næ•´ä½“çš„ç›®æ ‡å‡½æ•°ä¸º $$\\large L(\\theta) = \\max_{\\pi_\\theta} \\mathbb{E}{x \\sim D, y \\sim \\pi\\theta} [r(x, y)] - \\beta \\text{KL}[\\pi_\\theta(y|x), \\pi_{\\theta_{\\text{old}}}(y|x)]$$ï¼Œå‡½æ•°è¡¨è¾¾çš„æ„ä¹‰æ˜¯ï¼Œåœ¨ä¸€ä¸ªå¥–åŠ±ç»“æ„ä¸Šï¼Œpolicyçš„ç»“æ„å¿…é¡»å°½é‡ä¸å¥–åŠ±ç»“æ„ä¿æŒä¸€è‡´ï¼Œä½†åŒæ—¶ä¸è¦åç¦»è€çš„policyå¤ªè¿œï¼Œåè€…å¯ä»¥è®¤ä¸ºæ˜¯ä¸€å®šç¨‹åº¦çš„æ­£åˆ™åŒ–ã€‚ å½¢è±¡åŒ–ç†è§£ï¼Œå¯¹äº yå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒå’Œå¥–åŠ±åˆ†å¸ƒï¼ˆå³æ¨ªè½´æ˜¯y çš„å„ç§å–å€¼ï¼Œçºµè½´æ˜¯å¯¹åº”yå€¼è¡Œä¸ºçš„æ¦‚ç‡å€¼å¤§å°ã€å¥–åŠ±å¤§å°ï¼‰ï¼Œé‚£ä¹ˆç”¨å›¾å½¢ç†è§£å°±æ˜¯ä¸¤æ¡æ›²çº¿ï¼ˆåªä¸è¿‡ä¸€ä¸ªæ˜¯å½’ä¸€åŒ–çš„æ­£å€¼ï¼Œä¸€ä¸ªæ˜¯å¯èƒ½æ­£è´Ÿå€¼å‡å­˜åœ¨ï¼‰ã€‚å¯¹äºä¸Šè¿°ç›®æ ‡å‡½æ•°æ¥è¯´ï¼Œæš‚ä¸”è®¤ä¸ºå¥–åŠ±åˆ†å¸ƒ$r(x,y)$å°±æ˜¯çœŸå®çš„è®­ç»ƒæ•°æ®å€¼ï¼ˆä½†å…¶å®æ˜¯ä» æ­£è´Ÿä¾‹responseçš„ contrastive lossæ‹Ÿåˆå‡ºæ¥çš„å€¼ï¼‰ï¼Œé‚£ä¹ˆ$L(\\theta)$ä¸­çš„å”¯ä¸€å˜é‡å°±æ˜¯ ç­–ç•¥å‡½æ•° $\\pi_{\\theta}$ ï¼Œè¿™ä¸ªæ—¶å€™ç›®æ ‡å‡½æ•°$L(\\theta)$å…¶å®å°±æ˜¯ä¸€ä¸ªä»¥ $\\pi_\\theta$ä¸ºè‡ªå˜é‡çš„ä¸€ä¸ªå‡½æ•°ï¼ˆå½“ç„¶è‡ªå˜é‡æœ¬èº«å°±æ˜¯æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ï¼Œé‚£ä¹ˆè¿™å…¶å®å°±æ˜¯ä¸€ä¸ªæ³›å‡½ä¼˜åŒ–é—®é¢˜ï¼‰ï¼›å¯¹ä¸Šè¿°ç›®æ ‡å‡½æ•°è¿›è¡Œå˜æ¢ï¼Œå¯ä»¥å¾—å‡º $L(\\theta)$å–å¾—æœ€å¤§å€¼å¯¹åº”çš„è‡ªå˜é‡ $\\pi_\\theta$ æ˜¯æœ‰ä¸€ä¸ªå›ºå®šå…¬å¼çš„ï¼Œå³ $$\\large \\pi_r(y|x) = \\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_{\\theta_{\\text{old}}}(y|x) \\exp\\left(\\frac{1}{\\beta}r(x,y)\\right)$$ã€‚ è¿™é‡Œå¯ä»¥æç®€çš„æ–¹æ³•æ¨å¯¼å¤„ç†ï¼Œå¦‚ä¸‹ $$L =\\mathbb{E}[r]-KL = \\sum_{p} p*(r-\\log(\\frac{p}{q})) = \\sum_{p} p*(\\log(e^r)-\\log(\\frac{p}{q}))=\\sum_{p} p*(\\log(\\frac{e^rq}{p})) = - \\sum_{p} p(\\log(\\frac{e^rq}{p})) = - KL(p, e^rq)$$ï¼Œè€Œæœ€åä¸€ä¸ªå¼å­å°±æ˜¯KLæ•£åº¦çš„å…¬å¼å–è´Ÿæ•°ï¼ŒKLæ•£åº¦å…¬å¼å­˜åœ¨æœ€å°å€¼ï¼Œé‚£ä¹ˆ$L$å°±å­˜åœ¨æœ€å¤§å€¼ï¼Œæœ€ä¼˜ç‚¹å³ä¸º $\\large p = e^r*q$ï¼Œæœ€åå…¬å¼å¤–å›´å¥—ä¸ªå½’ä¸€åŒ–å°±æ˜¯æœ€ç»ˆçš„æ¨ç†ç»“æœã€‚ ç›´è§‰åŒ–çš„ç†è§£å°±æ˜¯ ç­–ç•¥å‡½æ•° -\u003e å’Œå¥–åŠ±ç»“æ„å¯¹åº”çš„æœ€ä¼˜ç­–ç•¥å‡½æ•° -\u003e å¾—åˆ°æœ€ä¼˜çš„æ•´ä½“ç›®æ ‡å€¼ã€‚è€Œå¥–åŠ±ç»“æ„å¿…é¡»ä¸çœŸå®çš„æ­£è´Ÿæ ·ä¾‹åå¥½ç»“æ„ä¸€è‡´ï¼Œæ‰€ä»¥æ•´ä½“ä¸Šå°±æ˜¯ å¥–åŠ±ç»“æ„å’Œç­–ç•¥å‡½æ•°æ˜¯ç»‘æ­»çš„æœ‰å›ºå®šå‡½æ•°å…³ç³»ï¼Œè€Œå¥–åŠ±ç»“æ„é€šè¿‡æŸå¤±å‡½æ•°ä¸çœŸå®æ­£è´Ÿæ ·ä¾‹åå¥½å¯¹é½ï¼Œé‚£ä¹ˆåå‘æ¥è¯´å°±æ˜¯ çœŸå®æ­£è´Ÿæ ·ä¾‹åå¥½-\u003eæŒ‡å¯¼å¥–åŠ±ç»“æ„è®¡ç®— -\u003eæŒ‡å¯¼å¯¹åº”çš„ç­–ç•¥å‡½æ•°ï¼Œ è€Œå¯¹åº”ç­–ç•¥å‡½æ•°å…¶å®æœ¬æ¥å°±åº”è¯¥è¾¾åˆ°ä¸€ç§æç«¯ç­–ç•¥ï¼ˆå³ä½¿å¾—å¥–åŠ±æœ€å¤§çš„responseçš„æ¦‚ç‡ç›´æ¥æ‹‰åˆ°æœ€å¤§å€¼1ï¼‰ï¼Œä½†å› ä¸ºæ­£åˆ™åŒ–çš„å­˜åœ¨ä½¿å¾— ç­–ç•¥å‡½æ•°æ˜¯ä»‹äº è€ç­–ç•¥å‡½æ•°å’Œ æç«¯ç­–ç•¥ ä¹‹é—´çš„ä¸­é—´ç­–ç•¥ã€‚ å‡å¦‚æ²¡æœ‰æ­£åˆ™åŒ–ï¼Œé‚£ä¹ˆæ•´ä¸ªè¿‡ç¨‹å°±æ˜¯æç«¯ç­–ç•¥ï¼Œè®©æ­£æ ·ä¾‹çš„ç”Ÿæˆæ¦‚ç‡æ— é™å¤§ï¼Œè´Ÿæ ·ä¾‹çš„ç”Ÿæˆæ¦‚ç‡æ— é™å°ï¼Œä¹Ÿå°±æ˜¯å¯¹åº”çš„logitsä¹‹å·®æ— é™å¤§ï¼Œé‚£ä¹ˆæ­£æ ·ä¾‹çš„æ¦‚ç‡ç›´æ¥ä¸º1ï¼Œè´Ÿæ ·ä¾‹çš„æ¦‚ç‡ç›´æ¥ä¸º0ï¼Œä½†è¿™æ ·å…¶å®å°±æ˜¯è¿‡æ‹Ÿåˆã€‚æ‰€ä»¥éœ€è¦æ­£åˆ™åŒ–æ¥é™åˆ¶ï¼Œé‚£ä¹ˆå°±æ˜¯é€šè¿‡ ç”¨æ¦‚ç‡çš„logitsæ¥è¡¨ç¤º $r(x,y)$ï¼ŒåŒæ—¶å‡å®šæŸå¤±ç»“æ„æ˜¯ è®© $r(x,y)$ å’Œ KLæ•£åº¦ç›´æ¥ç›¸åŠ ï¼Œæ¥ä½œä¸ºæœ€ç»ˆçš„ç›®æ ‡å‡½æ•°ã€‚ ç›´è§‚ä¸Šç†è§£DPOçš„å…¬å¼ï¼Œå³è®©æ­£åå¥½çš„å¯¹åº”çš„ç­–ç•¥actionæ¦‚ç‡è¶Šå¤§å¥½ï¼Œè®©è´Ÿåå¥½å¯¹åº”çš„ç­–ç•¥actionæ¦‚ç‡è¶Šå°è¶Šå¥½ã€‚ $$\\large L(\\theta)=-\\mathbb{E}{(x,y^+,y^-) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi{\\theta}(y^+|x)}{\\pi_{\\theta_{old}}(y^+|x)}-\\beta \\log \\frac{\\pi_{\\theta}(y^-|x)}{\\pi_{\\theta_{old}}(y^-|x)}\\right)\\right].$$ å…¶ä¸­$\\large \\hat{r}\\theta(x,y) = \\beta \\log \\left( \\frac{\\pi\\theta(y|x)}{\\pi_{\\theta_{old}}(y|x)} \\right)$ æ„æ€æ˜¯ï¼Œå¦‚æœå½“å‰ç­–ç•¥å¯¹åº”çš„é¢„ä¼°å¥–åŠ±æ˜¯å¤šå°‘ã€‚\nç›´è§‰ä¸Šæˆ‘å°†DPOæ¯”ä½œæ˜¯ä¸€ä¸ªä¸‰ä¸ªé“æ†ä¸Šåˆ†åˆ«å¥—ç€ä¸€ä¸ªç¯ï¼Œä¸‰ä¸ªé“ç¯ä¹‹é—´ æœ‰ä¸¤ä¸ªç»³å­è¿æ¥ï¼Œä¼ ç»Ÿçš„policy gradientæ˜¯å°†é€šè¿‡æ‹‰æœ€ä¸‹é¢ä¸€ä¸ªç¯ï¼Œè®©ä¸Šé¢ä¸¤ä¸ªé—´æ¥è¿å¸¦ç€ç§»åŠ¨ã€‚è€ŒDPOæ˜¯å°†ä¸Šé¢ä¸¤ä¸ªç¯å›ºåŒ–æˆä¸€ä½“ï¼Œåªè¦ç§»åŠ¨æœ€ä¸‹é¢ä¸€ä¸ªï¼Œå°±èƒ½è¾¾åˆ°ç›´æ¥ç§»åŠ¨æœ€ä¸Šé¢é“ç¯çš„ç›®çš„ã€‚\nDPOç®—æ³•çš„è®­ç»ƒæ ‡å‡†æ•°æ®ï¼Œéƒ½æ˜¯é…å¯¹çš„ã€‚\nDPOç®—æ³•ä¸­æŸå¤±å‡½æ•°ä¸­çš„logpæŒ‡çš„æ˜¯ä¸æ˜¯æ•´æ¡episodeä¸­ç­”æ¡ˆçš„æ¦‚ç‡logï¼Œå³åœ¨ç»™å®špromptçš„æƒ…å†µä¸‹ï¼Œç»™å‡ºresponseçš„æ¯ä¸ªtokenæ¦‚ç‡çš„ä¹˜ç§¯ï¼Œä¹Ÿå°±æ˜¯logpçš„å’Œã€‚\nGRPO DeepSeekMath è®ºæ–‡çš„è§£è¯» [[2025Q3-è®ºæ–‡å­¦ä¹ æ—¥è®°#2025-07-16 DeepSeekMath]] GRPO ä¸­ä½¿ç”¨äº†process supervision ä¿¡å·ï¼Œå³å•ä¸ªreponseçš„å¤šæ­¥æ¨ç†ï¼Œæ¯ä¸€æ­¥éƒ½æœ‰æ ‡æ³¨çš„rewardã€‚ç„¶å advantage å°±æ˜¯ å½’ä¸€åŒ–åçš„reward åœ¨tæ­¥ä¹‹åçš„ç´¯è®¡å’Œã€‚å³åŸºäºåŒä¸€ä¸ªpromptçš„æ‰€æœ‰responseçš„rewardçš„å‡å€¼å’Œæ–¹å·® è¿›è¡Œæ ‡å‡†åŒ– $\\Large \\tilde{r}i^{index(j)} = \\frac{r_i^{index(j)} - \\text{mean}(\\mathbf{R})}{\\text{std}(\\mathbf{R})}$ ï¼Œç„¶åç´¯è®¡å’Œ $\\Large \\hat{A}{i,t} = \\sum_{index(j) \\geq t} \\tilde{r}_i^{index(j)}$ ï¼Œè¿™é‡Œtå°±æ˜¯æ¯ä¸€ä¸ªtokenï¼Œjæ˜¯æ¯ä¸€ä¸ªreasoning stepï¼Œå³æ¯ä¸€ä¸ªtokenå…¶rewardå°±æ˜¯è¯¥tokenä¹‹åæ‰€æœ‰å¯¹åº”äº reasoning step end tokençš„tokençš„å¥–åŠ±ä¹‹å’Œã€‚æˆ‘æƒ³è¿™é‡Œä¹‹æ‰€ä»¥æ²¡æœ‰ä½¿ç”¨$\\gamma$ å¯èƒ½æ˜¯å› ä¸ºreasoning stepæ•°é‡æœ¬æ¥éƒ½æ˜¯æå…¶æœ‰é™çš„ï¼Œå¹¶ä¸”é‡‡æ ·åº”è¯¥ä¼šæ§åˆ¶å…¶æ•°é‡ã€‚ æ‰€ä»¥GRPOæ˜¯éœ€è¦ä½¿ç”¨ process reward model å¯¹responseçš„æ¯ä¸€ä¸ªæ¨ç†æ­¥éª¤è¿›è¡Œæ‰“åˆ†çš„ã€‚ The training data of the reward model is based on the rule judgment. Data Source: question in SFT dataset with outputs sampled from SFT model. Reward Function: Rule (whether the answer is correct or not) è®ºæ–‡é‡Œçš„å…¬å¼é‡Œæœ‰å¾ˆå¤šè®²ç©¶ã€‚\nåŸå§‹è®ºæ–‡ä¸­çš„å…¬å¼19\n$\\large \\mathcal{J}{\\text{GRPO}}(\\theta) = \\mathbb{E}{\\mathbf{q} \\sim P_{\\text{sft}}(Q), {o_i}{i=1}^G \\sim \\pi{\\theta_{old}}(O|q)} \\large \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\left[ \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,",
  "wordCount" : "9959",
  "inLanguage": "zh",
  "datePublished": "2025-08-22T00:00:00Z",
  "dateModified": "2025-08-22T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Chester"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://chesterwang.github.io/chester-blog/posts/2025-08-22-huggingface-%E7%B1%BB%E5%BA%93%E5%AD%A6%E4%B9%A0/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Chester's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://image.lvbibir.cn/blog/avatar.webp"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chesterwang.github.io/chester-blog/" accesskey="h" title="Chester&#39;s Blog (Alt + H)">
                <img src="https://image.lvbibir.cn/blog/avatar.webp" alt="" aria-label="logo"
                    height="35">Chester&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li class="menu-item">
                <a href="https://chesterwang.github.io/chester-blog/archives" title="ğŸ“ˆ å½’æ¡£">
                    <span>ğŸ“ˆ å½’æ¡£</span>
                    
                </a>
            
            </li>
            <li class="menu-item">
                <a href="https://chesterwang.github.io/chester-blog/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                    <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                    
                </a>
            
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://chesterwang.github.io/chester-blog/">ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://chesterwang.github.io/chester-blog/posts/">ğŸ“š æ–‡ç« </a></div>
            <h1 class="post-title">
                Huggingfaceåº“å­¦ä¹ ç¬”è®°
            </h1>
            <div class="post-description">
                Huggingfaceç›¸å…³Pythonåº“å­¦ä¹ ç¬”è®°
            </div>
            <div class="post-meta">åˆ›å»º: 2025-08-22 | æ›´æ–°: 2025-08-22 | å­—æ•°: 9959å­— | ä½œè€…:Chester


                |&nbsp;æ ‡ç­¾:&nbsp;
                <ul class="post-tags-meta">
                    <a href="https://chesterwang.github.io/chester-blog/tags/llm/">LLM</a>
                </ul>

                
                <span id="busuanzi_container_page_pv">
                    &nbsp;|&nbsp;è®¿é—®:&nbsp;<span id="busuanzi_value_page_pv"></span>
                </span>

</div>
        </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#hugging-face-hub" aria-label="Hugging Face Hub">Hugging Face Hub</a></li>
                <li>
                    <a href="#transformers" aria-label="transformers">transformers</a></li>
                <li>
                    <a href="#diffusers" aria-label="Diffusers">Diffusers</a></li>
                <li>
                    <a href="#sentence-transformers" aria-label="Sentence Transformers">Sentence Transformers</a></li>
                <li>
                    <a href="#trl" aria-label="trl">trl</a><ul>
                        <ul>
                        
                <li>
                    <a href="#ppo" aria-label="PPO">PPO</a></li>
                <li>
                    <a href="#dpo" aria-label="DPO">DPO</a></li>
                <li>
                    <a href="#grpo" aria-label="GRPO">GRPO</a></li>
                <li>
                    <a href="#gspo" aria-label="GSPO">GSPO</a>
                </li>
            </ul>
            </ul>
            </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><h2 id="hugging-face-hub">Hugging Face Hub<a hidden class="anchor" aria-hidden="true" href="#hugging-face-hub">#</a></h2>
<ol>
<li>Models, Spaces, and Datasets are hosted on the Hugging Face Hub asÂ <a href="https://git-scm.com/about" target="_blank" rel="noopener" style="color:#42b983";>Git repositories</a></li>
<li>Do you have files larger than 10MB? Those files should be tracked withÂ <code>git-lfs</code>, which you can initialize with: <code>git lfs install</code></li>
<li>Note that if your files are larger thanÂ <strong>5GB</strong>Â youâ€™ll also need to run: <code>hf lfs-enable-largefiles .</code></li>
<li>Pull Request ä¹‹æ‰€ä»¥å«è¿™ä¸ªåå­—ï¼Œæ˜¯å› ä¸ºå®ƒå‡†ç¡®åœ°æè¿°äº†è¯·æ±‚è€…ï¼ˆä½ ï¼‰å’Œè¢«è¯·æ±‚è€…ï¼ˆé¡¹ç›®ç»´æŠ¤è€…ï¼‰ä¹‹é—´çš„åŠ¨ä½œå’Œæ–¹å‘ã€‚
<ol>
<li>PR = â€œæˆ‘æ”¹å¥½äº†ï¼Œè¯·ä½ æŠŠæˆ‘è¿™è¾¹çš„ä¿®æ”¹æ‹‰ï¼ˆpullï¼‰è¿›ä½ çš„ä¸»åˆ†æ”¯å§ã€‚â€è¿™ä¸ª â€œpullâ€ å¹¶ä¸æ˜¯æŒ‡ä½ è‡ªå·±å»æ‹‰ï¼Œè€Œæ˜¯ <strong>è¯·æ±‚é¡¹ç›®ç»´æŠ¤è€…å»æ‹‰ä½ çš„ä»£ç </strong>ã€‚</li>
</ol>
</li>
<li><a href="https://huggingface.co/docs/transformers/en/chat_templating" target="_blank" rel="noopener" style="color:#42b983";>Templates</a></li>
</ol>
<h2 id="transformers">transformers<a hidden class="anchor" aria-hidden="true" href="#transformers">#</a></h2>
<ol>
<li>quickstart
<ol>
<li>load a pretrained model</li>
<li>run inference withÂ <a href="https://huggingface.co/docs/transformers/v4.53.3/en/main_classes/pipelines#transformers.Pipeline" target="_blank" rel="noopener" style="color:#42b983";>Pipeline</a></li>
<li>fine-tune a model withÂ <a href="https://huggingface.co/docs/transformers/v4.53.3/en/main_classes/trainer#transformers.Trainer" target="_blank" rel="noopener" style="color:#42b983";>Trainer</a></li>
</ol>
</li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/auto" target="_blank" rel="noopener" style="color:#42b983";>Auto Classes</a>
<ol>
<li>Instantiating one ofÂ <a href="https://huggingface.co/docs/transformers/v4.53.3/en/model_doc/auto#transformers.AutoConfig" target="_blank" rel="noopener" style="color:#42b983";>AutoConfig</a>,Â <a href="https://huggingface.co/docs/transformers/v4.53.3/en/model_doc/auto#transformers.AutoModel" target="_blank" rel="noopener" style="color:#42b983";>AutoModel</a>, andÂ <a href="https://huggingface.co/docs/transformers/v4.53.3/en/model_doc/auto#transformers.AutoTokenizer" target="_blank" rel="noopener" style="color:#42b983";>AutoTokenizer</a>Â will directly create a class of the relevant architecture.</li>
</ol>
</li>
<li>transformersåº“ä¸­ AutoImageProcessor å®ä¾‹è¯å‡ºæ¥çš„processor çš„ä½œç”¨éƒ½æœ‰å“ªäº›
<ol>
<li>ä¸€æ—¦ä½ é€šè¿‡ <code>AutoImageProcessor.from_pretrained()</code> å®ä¾‹åŒ–äº†ä¸€ä¸ªå¤„ç†å™¨ï¼Œè¿™ä¸ª <strong>processor å®ä¾‹</strong> å°±æˆä¸ºäº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œä¸“é—¨ç”¨äºå‡†å¤‡å›¾åƒæ•°æ®ï¼Œä½¿å…¶èƒ½å¤Ÿè¢«ç‰¹å®šçš„é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ä½¿ç”¨ã€‚å®ƒçš„ä¸»è¦ä½œç”¨å¯ä»¥æ¦‚æ‹¬ä¸ºä»¥ä¸‹å‡ ç‚¹ï¼š</li>
<li>å›¾åƒæ ‡å‡†åŒ– (Normalization)</li>
<li>å›¾åƒå°ºå¯¸è°ƒæ•´ (Resizing) å’Œè£å‰ª (Cropping)</li>
<li>é€šé“æ ¼å¼è½¬æ¢ (Channel Format Conversion)</li>
<li>å›¾åƒåˆ°å¼ é‡è½¬æ¢ (Image to Tensor Conversion)</li>
<li>æ‰¹å¤„ç† (Batching)</li>
<li>æ•°æ®å¢å¼º (Data Augmentation)</li>
</ol>
</li>
<li>transoformeråº“ä¸­ TFAutoModelå’ŒAutoModelçš„åŒºåˆ«æ˜¯ä»€ä¹ˆ
<ol>
<li><strong><code>AutoModel</code></strong>ï¼šç”¨äºåŠ è½½ <strong>PyTorch</strong> æ¡†æ¶ä¸‹çš„æ¨¡å‹ã€‚</li>
<li><strong><code>TFAutoModel</code></strong>ï¼šç”¨äºåŠ è½½ <strong>TensorFlow 2.0</strong> æ¡†æ¶ä¸‹çš„æ¨¡å‹ã€‚</li>
<li><strong><code>FlaxAutoModel</code></strong>: ç”¨äºåŠ è½½ <strong>Flax</strong>ï¼ˆåŸºäº JAX çš„æ¡†æ¶ï¼‰ä¸‹çš„æ¨¡å‹ã€‚</li>
</ol>
</li>
<li>AutoModel ç±»çš„åç¼€
<ol>
<li>LM
<ol>
<li>CausalLM</li>
<li>MaskedLM</li>
<li>MaskedGeneration</li>
<li>sequenceClassification</li>
<li>TokenClassification</li>
<li>NextSentencePrediction</li>
<li>MultipleChoice</li>
<li>Seq2SeqLM</li>
<li>QuestionAnswering</li>
</ol>
</li>
</ol>
</li>
<li>Backbone
<ol>
<li>A backbone is a model used for feature extraction for higher level computer vision tasks such as object detection and image classification.</li>
</ol>
</li>
<li>Data Collator
<ol>
<li>Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements ofÂ <code>train_dataset</code>Â orÂ <code>eval_dataset</code>.</li>
<li>æˆ‘ç†è§£ Data Collator å°±æ˜¯æŠŠå¤šä¸ªåˆ—æ”¾åœ¨ä¸€èµ·ï¼Œå°±æ˜¯collatorçš„å­—é¢æ„æ€ï¼Œä½†å…¶å®å†…éƒ¨è¿˜æ˜¯ä¼šæœ‰ä¸€äº›å…·ä½“çš„æ•°æ®å¤„ç†é€»è¾‘ï¼Œæ¯”å¦‚paddingã€æ•°æ®æ‰©å¢ç­‰</li>
</ol>
</li>
<li>pipeline
<ol>
<li><code>from transformers import pipeline</code></li>
<li><code>generator = pipeline(model=&quot;openai-community/gpt2&quot;)</code></li>
<li><code>generator(&quot;I can't believe you did such a &quot;, do_sample=False)</code></li>
<li><code>[{'generated_text': &quot;I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I&quot;}]</code></li>
</ol>
</li>
<li>chat with models
<ol>
<li>Templates</li>
</ol>
</li>
</ol>
<h2 id="diffusers">Diffusers<a hidden class="anchor" aria-hidden="true" href="#diffusers">#</a></h2>
<ol>
<li>LoRA
<ol>
<li>Add a LoRA to a pipeline with theÂ <a href="https://huggingface.co/docs/diffusers/v0.35.1/en/api/loaders/lora#diffusers.loaders.QwenImageLoraLoaderMixin.load_lora_weights" target="_blank" rel="noopener" style="color:#42b983";>load_lora_weights()</a>Â method. Some LoRAâ€™s require a special word to trigger it, such asÂ <code>Realism</code>, in the example below. Check a LoRAâ€™s model card to see if it requires a trigger word.</li>
<li>LoRAæ–‡ä»¶å°±æ˜¯ä¸€ç§æ’ä»¶</li>
</ol>
</li>
</ol>
<h2 id="sentence-transformers">Sentence Transformers<a hidden class="anchor" aria-hidden="true" href="#sentence-transformers">#</a></h2>
<p>sentence embedding demo</p>
<div class="highlight"><pre tabindex="0" style="color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#5bc4bf">from</span> <span style="color:#fec418">sentence_transformers</span> <span style="color:#5bc4bf">import</span> SentenceTransformer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#776e71"># 1. Load a pretrained Sentence Transformer model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#5bc4bf">=</span> SentenceTransformer(<span style="color:#48b685">&#34;all-MiniLM-L6-v2&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#776e71"># The sentences to encode</span>
</span></span><span style="display:flex;"><span>sentences <span style="color:#5bc4bf">=</span> [
</span></span><span style="display:flex;"><span>    <span style="color:#48b685">&#34;The weather is lovely today.&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#48b685">&#34;It&#39;s so sunny outside!&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#48b685">&#34;He drove to the stadium.&#34;</span>,
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#776e71"># 2. Calculate embeddings by calling model.encode()</span>
</span></span><span style="display:flex;"><span>embeddings <span style="color:#5bc4bf">=</span> model<span style="color:#5bc4bf">.</span>encode(sentences)
</span></span><span style="display:flex;"><span>print(embeddings<span style="color:#5bc4bf">.</span>shape)
</span></span><span style="display:flex;"><span><span style="color:#776e71"># [3, 384]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#776e71"># 3. Calculate the embedding similarities</span>
</span></span><span style="display:flex;"><span>similarities <span style="color:#5bc4bf">=</span> model<span style="color:#5bc4bf">.</span>similarity(embeddings, embeddings)
</span></span><span style="display:flex;"><span>print(similarities)
</span></span><span style="display:flex;"><span><span style="color:#776e71"># tensor([[1.0000, 0.6660, 0.1046],</span>
</span></span><span style="display:flex;"><span><span style="color:#776e71">#         [0.6660, 1.0000, 0.1411],</span>
</span></span><span style="display:flex;"><span><span style="color:#776e71">#         [0.1046, 0.1411, 1.0000]])</span>
</span></span></code></pre></div><hr>
<h2 id="trl">trl<a hidden class="anchor" aria-hidden="true" href="#trl">#</a></h2>
<ol>
<li><a href="https://huggingface.co/docs/trl/quickstart" target="_blank" rel="noopener" style="color:#42b983";>Quickstart</a> ç®€å•çš„helloworldç¨‹åºã€‚
<ol>
<li><strong>evaluation</strong>: The important thing is that this process should yield a scalar value for each query/response pair.</li>
<li>è¿™é‡Œçš„ä¾‹å­ä»£ç åº”è¯¥å·²ç»è¿‡æ—¶äº†ï¼ŒPPOTrainerç°åœ¨æ²¡æœ‰stepè¿™ä¸ªæ–¹æ³•ã€‚</li>
</ol>
</li>
<li><a href="https://huggingface.co/docs/trl/dataset_formats" target="_blank" rel="noopener" style="color:#42b983";>Dataset formats and types</a></li>
<li>How-to guides
<ol>
<li>customozing the training
<ol>
<li>Memory efficient fine-tuning by sharing layers</li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="ppo">PPO<a hidden class="anchor" aria-hidden="true" href="#ppo">#</a></h4>
<ol>
<li>PPOç­–ç•¥ä¸­çš„ä¸€äº›åŸºç¡€çŸ¥è¯†
<ol>
<li>æ·±åº¦å­¦ä¹ ç¡¬ä»¶é…ç½®ä¸­çš„æ¦‚å¿µ device rank world_size node åˆ†åˆ«æ˜¯æŒ‡ä»€ä¹ˆ
<ol>
<li>è¿™å››ä¸ªæ¦‚å¿µçš„å…³ç³»å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š<strong>node</strong> æ˜¯ç‰©ç†æœºå™¨ã€‚ ä¸€ä¸ª <strong>node</strong> å¯ä»¥åŒ…å«å¤šä¸ª <strong>device</strong>ï¼Œdeviceé€šå¸¸æ˜¯æŒ‡GPUã€‚ æ¯ä¸ª <strong>device</strong> é€šå¸¸ç”±ä¸€ä¸ªç‹¬ç«‹çš„ <strong>rank</strong> è¿›ç¨‹æ¥æ§åˆ¶ã€‚ æ‰€æœ‰ <strong>rank</strong> è¿›ç¨‹çš„æ€»æ•°å°±æ˜¯ <strong>world_size</strong>ã€‚</li>
</ol>
</li>
<li>PPOç¤ºä¾‹
<ol>
<li><a href="https://github.com/huggingface/trl/blob/v0.21.0/examples/scripts/ppo/ppo.py" target="_blank" rel="noopener" style="color:#42b983";>trl/examples/scripts/ppo/ppo.py at v0.21.0 Â· huggingface/trl</a></li>
<li>model=policy, ref_model=ref_policy, reward_model=reward_model, value_model=value_model,ã€</li>
<li>reward_modelå…¶å®æ˜¯æ¯ä¸€å°æ­¥å†³ç­–çš„å³æ—¶çš„ã€ç›´æ¥å¥–åŠ±ï¼Œvalue_modelå¯¹æ¯ä¸€å°æ­¥å†³ç­–çš„å…¨å±€æ€§ã€é•¿æœŸæ€§åæœè¿›è¡Œé¢„æµ‹ã€‚</li>
<li>advantage æˆ‘ç›´è§‚ç†è§£æ˜¯å› ä¸ºQ(s,a) è¿™ä¸ªstateçš„éšæœºé€‰æ‹©å¯¼è‡´åç»­çš„æ‰€æœ‰rewardéƒ½æ¯”è¾ƒå¼‚å¸¸ï¼Œæ‰€ä»¥è¦æ¶ˆé™¤ä¸€éƒ¨åˆ†stateçš„éšæœºæ€§ï¼Œæ‰€ä»¥å‡å»stateçš„valueï¼›å¦‚æœè®­ç»ƒæ•°æ®ä¸­çš„Q(s,a)ä¸­çš„sè¶³å¤Ÿå¤šï¼Œé‚£ä¹ˆå°±å¯ä»¥ç›´æ¥ä½¿ç”¨Qï¼Œä½†å› ä¸ºä¸å¤Ÿå¤šæœ‰äº†éšæœºæ€§ï¼Œæ‰€ä»¥Qä»£è¡¨çš„returnåˆ†å¸ƒå°±åç¦»äº†æ‰€è°“çš„å…¨å±€æ€§çš„Qçš„åˆ†å¸ƒï¼Œæ‰€ä»¥é€šè¿‡å‡å»è¿™ä¸ªsçš„éšæœºæ€§ä»è€Œæ‹‰å›æ­£å¸¸åˆ†å¸ƒã€‚</li>
</ol>
</li>
<li>Mixin ç±»æœ¬èº«ä¸èƒ½ç‹¬ç«‹å®ä¾‹åŒ–ï¼Œå®ƒä¸æ˜¯ç”¨æ¥åˆ›å»ºå¯¹è±¡çš„ã€‚ç›¸åï¼Œå®ƒåƒä¸€ä¸ªâ€œåŠŸèƒ½åŒ…â€æˆ–â€œèƒ½åŠ›æ’ä»¶â€ï¼Œä¸“é—¨è®¾è®¡ç”¨æ¥è¢«å…¶ä»–ç±»<strong>ç»§æ‰¿</strong>ã€‚å½“ä¸€ä¸ªç±»ç»§æ‰¿äº†ä¸€ä¸ªæˆ–å¤šä¸ª Mixin ç±»æ—¶ï¼Œå®ƒå°±è‡ªåŠ¨è·å¾—äº†è¿™äº› Mixin ç±»æ‰€å®šä¹‰çš„æ‰€æœ‰æ–¹æ³•å’Œå±æ€§ã€‚</li>
<li>PPOä¸­çš„æ•°æ®å¤„ç†
<ol>
<li><code>transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus</code> æ–¹æ³•ä¸­åŒ…å«äº† tokenizerä¸­çš„paddingå¤„ç†ã€‚</li>
<li><a href="https://github.com/huggingface/trl/blob/v0.21.0/examples/scripts/ppo/ppo.py" target="_blank" rel="noopener" style="color:#42b983";>PPOå®˜æ–¹ä¾‹å­ä¸­</a>ä¸­çš„ä»£ç ï¼Œ
<ol>
<li>ä¸ºä»€ä¹ˆ padding_side å’Œåé¢çš„padding=false ï¼Œè¿™ä¸¤è€…ä¸å†²çªä¹ˆ
<ol>
<li>ä»£ç ä¸­ä¸¤ä¸ªåœ°æ–¹ä½¿ç”¨äº†tokenizerï¼Œ
<ol>
<li><code>PPOTrainer( args=training_args, processing_class=tokenizer,...)</code> è¿™é‡Œçš„tokenizerå®šä¹‰ä¸­æ˜¯å¸¦æœ‰padding=&ldquo;left&quot;çš„ã€‚
<ol>
<li>è¿™é‡Œçš„tokenizer åœ¨PPOTrainerä¸­æ˜¯ä½œä¸º processing_class çš„ä½œç”¨ã€‚</li>
<li>åœ¨PPOTrainerä¸­ ä¼šæ ¹æ® processing_class ç”Ÿæˆä¸€ä¸ª DataCollatorWithPadding ç±»ç”¨äºæ•°æ®è¿›è¡Œå¤„ç†ï¼Œè€Œè¯¥ç±»çš„è®¡ç®—é€»è¾‘æ˜¯ä½¿ç”¨tokenizerå¯¹æ•°æ®ä»…ä»…è¿›è¡Œpaddingæ“ä½œï¼ˆè¿™é‡Œçš„paddingæ˜¯å³å¯¹é½ï¼‰ï¼Œè¿™é‡Œä¸åŒ…æ‹¬encodingæ“ä½œã€‚å› ä¸ºè®­ç»ƒæ•°æ®å·²ç»æ ¹æ®ç¬¬äºŒæ¡ä¸­çš„ prepare_datasetè¿›è¡Œäº†encodingã€‚</li>
</ol>
</li>
<li>prepare_dataset ä¸­çš„ tokenizeå‡½æ•°ã€‚
<ol>
<li>è¯¥å‡½æ•°çš„è¾“å…¥æ˜¯ä¸€æ¡æ•°æ®ï¼Œæ‰€ä»¥ä¸éœ€è¦è¿›è¡Œpaddingã€‚</li>
</ol>
</li>
<li>æ€»ç»“ï¼š å³encoddingå’Œpaddingæ˜¯åˆ†åœ¨ä¸¤ä¸ªåœ°æ–¹è¿›è¡Œå¤„ç†çš„ï¼Œåˆ†åˆ«æ˜¯PPOTrainerå®ä¾‹åŒ–ä¹‹å‰ å’Œ PPOTrainerçš„trainæ–¹æ³•å†…éƒ¨ åˆ†åˆ«è¿›è¡Œencodingå’Œpaddingã€‚</li>
</ol>
</li>
<li>è‡³äºä¸ºä»€ä¹ˆè¿™æ ·åšï¼Œä»ä»£ç ä¸­æ²¡æœ‰çœ‹å‡ºåŸå› ã€‚å¯èƒ½æ˜¯ä¸ºäº†æ›´ç»†ç²’åº¦çš„æ§åˆ¶è®­ç»ƒå’Œè¯„ä¼°å„è‡ªåœºæ™¯ä¸‹çš„é€»è¾‘ã€‚</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><code>trl.trainer.utils.selective_log_softmax</code>
<ol>
<li>å…¶ä¸­å®ç°é’ˆå¯¹FP32 FP64 é‡‡ç”¨äº†é«˜æ•ˆçš„ logsumexpæ–¹æ³•ï¼Œé’ˆå¯¹å…¶ä»–çš„æ ¼å¼é‡‡ç”¨äº†ä½æ•ˆçš„æ–¹æ³•ï¼Œå› ä¸ºä½ç²¾åº¦æ ¼å¼è®¡ç®—å®¹æ˜“å‡ºç°é—®é¢˜ã€‚</li>
<li><code>logsumexp</code> çš„æ ‡å‡†å®šä¹‰æ˜¯ï¼š $$\log \sum_{i} \exp(x_i)$$ï¼Œå¦‚æœç›´æ¥æŒ‰è¿™ä¸ªå…¬å¼å®ç°ï¼Œå½“ $x_i$ çš„å€¼å¾ˆå¤§æ—¶ï¼Œ$\exp(x_i)$ å¯èƒ½ä¼šå¯¼è‡´æ•°å€¼<strong>ä¸Šæº¢ï¼ˆoverflowï¼‰</strong>ï¼Œè¶…å‡ºæµ®ç‚¹æ•°çš„è¡¨ç¤ºèŒƒå›´ï¼Œç»“æœå˜ä¸ºæ— ç©·å¤§ã€‚</li>
<li>å·¥ç¨‹ä¸Šçš„ç¨³å®šå®ç°æ–¹æ³•
1. ä¸ºäº†é¿å…ä¸Šæº¢ï¼Œ<code>logsumexp</code> åœ¨å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚ PyTorchã€TensorFlowï¼‰ä¸­éƒ½æœ‰ä¸€ä¸ªç»è¿‡ä¼˜åŒ–çš„ã€æ•°å€¼ç¨³å®šçš„å®ç°ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å¯¹æ•°å‡½æ•°çš„æ€§è´¨ï¼Œå°†æŒ‡æ•°è¿ç®—ä¸­çš„å¤§æ•°ç›¸åŠ é—®é¢˜ï¼Œè½¬æ¢ä¸ºå¯¹æ•°è¿ç®—ä¸­çš„å°èŒƒå›´æ•°ç›¸åŠ é—®é¢˜ã€‚
2. å…·ä½“æ–¹æ³•å¦‚ä¸‹ï¼š
<ol>
<li><strong>æ‰¾åˆ°æœ€å¤§å€¼</strong>ï¼šé¦–å…ˆï¼Œæ‰¾åˆ°è¾“å…¥å‘é‡ $x$ ä¸­çš„æœ€å¤§å€¼ $x_{max}$ã€‚</li>
<li><strong>è½¬æ¢å…¬å¼</strong>ï¼šåˆ©ç”¨ $e^{a+b} = e^a e^b$ çš„æ€§è´¨ï¼Œå°†åŸå…¬å¼è¿›è¡Œç­‰ä»·è½¬æ¢ï¼š $$\log \sum_{i} \exp(x_i) = \log \left( \exp(x_{max}) \sum_{i} \exp(x_i - x_{max}) \right)$$</li>
<li><strong>æ‹†åˆ†å¯¹æ•°</strong>ï¼šåˆ©ç”¨ $\log(ab) = \log(a) + \log(b)$ çš„æ€§è´¨ï¼Œè¿›ä¸€æ­¥æ‹†åˆ†ï¼š $$= \log(\exp(x_{max})) + \log \left( \sum_{i} \exp(x_i - x_{max}) \right)$$ $$= x_{max} + \log \left( \sum_{i} \exp(x_i - x_{max}) \right)$$</li>
<li>ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ³•æ›´ç¨³å®šï¼Ÿ
<ol>
<li><strong>é˜²æ­¢ä¸Šæº¢</strong>ï¼šåœ¨è½¬æ¢åçš„å…¬å¼ä¸­ï¼Œ$x_i - x_{max}$ çš„å€¼éƒ½æ˜¯<strong>è´Ÿæ•°æˆ–è€…é›¶</strong>ã€‚è¿™æ„å‘³ç€ $\exp(x_i - x_{max})$ çš„å€¼éƒ½åœ¨ $(0, 1]$ èŒƒå›´å†…ã€‚è¿™æ ·ï¼Œå³ä½¿ $x_i$ éå¸¸å¤§ï¼ŒæŒ‡æ•°è¿ç®—çš„ç»“æœä¹Ÿä¸ä¼šä¸Šæº¢ã€‚</li>
<li><strong>ä¿æŒç²¾åº¦</strong>ï¼šè™½ç„¶ $x_i - x_{max}$ çš„å€¼æ˜¯è´Ÿæ•°ï¼Œä½†å®ƒä»¬ä¹‹é—´çš„ç›¸å¯¹å¤§å°å…³ç³»ä¿æŒä¸å˜ï¼Œè¿™ä¿è¯äº†è®¡ç®—ç»“æœçš„ç²¾ç¡®åº¦ã€‚</li>
<li><strong>è®¡ç®—æ•ˆç‡</strong>ï¼šè¿™ä¸ªç¨³å®šçš„å®ç°åªéœ€è¦é¢å¤–è¿›è¡Œä¸€æ¬¡ <code>max</code> è¿ç®—å’Œä¸€æ¬¡åŠ æ³•è¿ç®—ï¼Œå¯¹æ•´ä½“è®¡ç®—æ•ˆç‡å½±å“å¾ˆå°ã€‚</li>
<li>ä¾‹å¦‚ï¼Œåœ¨ PyTorch ä¸­ï¼Œ<code>torch.logsumexp</code> å‡½æ•°å°±æ˜¯ä»¥è¿™ç§æ–¹å¼å®ç°çš„ã€‚å½“ä½ ä½¿ç”¨å®ƒæ—¶ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨å¤„ç†è¿™äº›æ•°å€¼ç¨³å®šæ€§çš„ç»†èŠ‚ã€‚</li>
<li>è¿™ä¸ªå·¥ç¨‹ä¼˜åŒ–æ–¹æ³•åœ¨æ·±åº¦å­¦ä¹ ä¸­éå¸¸é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¯¸å¦‚ Softmax äº¤å‰ç†µæŸå¤±ã€ä¿¡å¿µä¼ æ’­ï¼ˆBelief Propagationï¼‰ç­‰éœ€è¦å¤§é‡æŒ‡æ•°å’Œå¯¹æ•°è¿ç®—çš„åœºæ™¯ã€‚</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><strong>PPOç­–ç•¥çš„æµç¨‹(from chatgpt)</strong> ^PPO-process
<ol>
<li><strong>ä¸€ã€PPOç®—æ³•æ ¸å¿ƒæ€æƒ³</strong>
<ol>
<li>PPO å±äº**ç­–ç•¥æ¢¯åº¦ï¼ˆPolicy Gradientï¼‰**å®¶æ—ï¼Œç›®æ ‡æ˜¯é€šè¿‡ä¸æ–­ä¼˜åŒ–ç­–ç•¥å‚æ•°ï¼Œä½¿å¾—æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è·å¾—æ›´é«˜çš„æœŸæœ›å›æŠ¥ã€‚</li>
<li>å®ƒçš„å…³é”®åœ¨äºï¼š<strong>åœ¨æ›´æ–°ç­–ç•¥æ—¶é™åˆ¶æ›´æ–°å¹…åº¦</strong>ï¼Œé˜²æ­¢ç­–ç•¥æ”¹å˜å¤ªå¤§å¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚</li>
</ol>
</li>
<li><strong>äºŒã€PPOçš„åŸºæœ¬æµç¨‹ï¼ˆå…¸å‹ç‰ˆæœ¬ï¼šPPO-Clipï¼‰</strong>
<ol>
<li><strong>é‡‡æ ·ï¼ˆRolloutï¼‰</strong>
<ol>
<li>ä½¿ç”¨å½“å‰ç­–ç•¥ $\pi_{\theta_{old}}(a_t|s_t)$ï¼Œä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ä¸€æ‰¹æ•°æ®ï¼š $(s_t, a_t, r_t, s_{t+1})$</li>
<li>å¹¶è®¡ç®—æŠ˜æ‰£å›æŠ¥ $R_t$ ã€‚</li>
</ol>
</li>
<li><strong>è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantage Estimateï¼‰</strong>
<ol>
<li>é€šå¸¸ç”¨ <strong>GAEï¼ˆGeneralized Advantage Estimationï¼‰</strong>ï¼š   $\hat{A}<em>t = \sum</em>{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}$</li>
<li>å…¶ä¸­ $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ã€‚</li>
</ol>
</li>
<li><strong>è®¡ç®—é‡è¦æ€§æ¯”ç‡ï¼ˆratioï¼‰</strong>
<ol>
<li>$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$    è¡¨ç¤ºæ–°æ—§ç­–ç•¥åœ¨åŒä¸€åŠ¨ä½œä¸Šçš„â€œæ¦‚ç‡å˜åŒ–â€ã€‚</li>
</ol>
</li>
<li><strong>æ„å»ºPPOçš„ç›®æ ‡å‡½æ•°ï¼ˆClipped Surrogate Objectiveï¼‰</strong>
<ol>
<li>$\large L^{CLIP}(\theta) = \mathbb{E}_t \big[  \min(  r_t(\theta) \hat{A}_t,  \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t  )\big]$</li>
<li>å½“  $r_t(\theta)$  åç¦» 1 å¤ªå¤šï¼ˆè¶…è¿‡ Â±Îµï¼‰æ—¶ï¼Œä¼šè¢«æˆªæ–­ï¼ˆclipï¼‰ï¼Œé˜²æ­¢è¿‡åº¦æ›´æ–°ã€‚ Îµ é€šå¸¸å– 0.1ï½0.2ã€‚</li>
</ol>
</li>
<li><strong>ä¼˜åŒ–ç›®æ ‡ + å€¼å‡½æ•° + ç†µæ­£åˆ™</strong>
<ol>
<li>å®é™…ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°é€šå¸¸æ˜¯ä¸‰é¡¹ä¹‹å’Œï¼š   $L(\theta) = L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S[\pi_\theta]$</li>
<li>$L^{VF}$ï¼šå€¼å‡½æ•°çš„MSEæŸå¤±</li>
<li>$S[\pi_\theta]$ï¼šç­–ç•¥çš„ç†µï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰</li>
</ol>
</li>
<li><strong>å¤šæ¬¡å°æ­¥æ›´æ–°ï¼ˆK epochsï¼‰</strong>
<ol>
<li>ç”¨åŒä¸€æ‰¹é‡‡æ ·æ•°æ®ï¼Œåœ¨æ¯ä¸ª mini-batch ä¸Šä¼˜åŒ– K è½®ã€‚</li>
</ol>
</li>
<li><strong>æ›´æ–°æ—§ç­–ç•¥å‚æ•°</strong>
<ol>
<li>$\theta_{old} \leftarrow \theta$</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<blockquote>
<p><strong>PPOTrainerä¸­ä»£ç çš„æµç¨‹æ€»ç»“</strong> ^PPO-code-process</p>
<ol>
<li><strong>å¤–å¾ªç¯</strong>ï¼šæ€»è®­ç»ƒbatchæ•°
<ol>
<li>å¯¹ policy model ç”Ÿæˆçš„æ¯ä¸€ä¸ªåºåˆ—æ ·æœ¬ï¼Œè¾“å…¥åˆ°reward model ï¼Œåºåˆ—ä¸­æœ€åä¸€ä¸ªtokençš„ hidden state è¢«è¾“å…¥åˆ°ä¸€ä¸ªçº¿æ€§å¤´ï¼Œæœ€ç»ˆè¾“å‡ºä¸€ä¸ªæ ‡é‡å€¼ï¼Œè¡¨ç¤ºåºåˆ—äºŒåˆ†ç±»çš„æ‰“åˆ†scoreã€‚</li>
<li>å¯¹ policy model ç”Ÿæˆçš„æ¯ä¸€ä¸ªåºåˆ—æ ·æœ¬ï¼Œæ¯ä¸€ä¸ªtokenè¾“å‡ºå‰çš„vocabulary åˆ†å¸ƒå’Œ ref_modelçš„åˆ†å¸ƒ ä¸¤è€…è®¡ç®—klæ•£åº¦ï¼ˆä½¿ç”¨äº†è¿‘ä¼¼æ–¹æ³•ä½¿å¾—è®¡ç®—åŠ é€Ÿï¼‰ï¼Œå³è¾“å‡ºä¸€ä¸ª klæ•£åº¦åºåˆ—ï¼Œåºåˆ—ä¸­æ¯ä¸ªå…ƒç´ è¡¨ç¤ºå¯¹åº”ä½ç½®tokençš„klæ•£åº¦ã€‚</li>
<li>å°†åºåˆ—çº§çš„score åŠ åˆ°klåºåˆ—ä¸­æ¯ä¸€ä¸ªä½ç½®ï¼Œç»“æœå³ä¸º rewards åºåˆ—</li>
<li>ä¾æ¬¡è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ gaeï¼Œ<code>GAE(t)=rewards(t) + gamma*value(t+1) - value(t)   + gamma*lam*GAE(t+1)</code>ï¼Œè¿™é‡Œlamæ˜¯æŒ‡lambda</li>
<li>è®¡ç®— return <code>return(t) =  GAE(t)+ value(t)</code> ã€‚ï¼ˆè¿™é‡ŒæŠŠvalueç†è§£æˆæ˜¯ tæ­¥çš„çŠ¶æ€stateä¸‹ï¼Œæœªæ¥å¥–åŠ±çš„æ€»å’ŒæœŸæœ›ï¼Œå³ çŠ¶æ€æœ¬èº«çš„ä»·å€¼ï¼‰ã€‚</li>
<li>valueå³ state value functionï¼ŒGAEå³ é’ˆå¯¹çš„æ˜¯actionã€‚</li>
<li><strong>å†…å¾ªç¯</strong>ï¼šé’ˆå¯¹ä¸Šè¿°æ­¥éª¤ç”Ÿæˆçš„åºåˆ—æ ·æœ¬å’Œå¯¹åº”çš„returnã€GAEã€æ¦‚ç‡logp åºåˆ—ï¼Œå°†æ ·æœ¬æ‹†åˆ†æˆ micro batchï¼Œæ‰§è¡Œä¸‹åˆ—æ­¥éª¤
<ol>
<li>value ä¼˜åŒ–çš„æŸå¤±å‡½æ•°ä¸º <code>(return - value(t))^2</code>ï¼Œå…¶å®å°±æ˜¯çº¦æŸ ä¼˜åŠ¿advantage GAE æœ¬èº«å°½é‡ã€‚</li>
<li>ä½¿ç”¨å½“å‰çš„policy model è®¡ç®—action çš„æ–°æ¦‚ç‡å€¼ $\large \log p_{\theta}$</li>
<li>policy gradientæŸå¤±ä¸º $\Large - GAE * exp(\log p_{\theta}-\log p_{\theta_{old}})$ æˆ‘æƒ³è¿™é‡Œè¿™ä¹ˆå†™æ˜¯ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œå› ä¸ºä¸€èˆ¬æ˜¯è®©æ¨¡å‹è¾“å‡º logpï¼Œç„¶åè®¡ç®— æ¦‚ç‡æ¯”ï¼Œé‚£ä¹ˆå°±ç›´æ¥ç›¸å‡ç„¶åå–æŒ‡æ•°ã€‚èƒŒåå…¶å®å°±æ˜¯ logsumexpç®—å­ã€‚</li>
<li>ä¸¤ä¸ªæŸå¤±åŠ è½½ä¸€èµ·è¿›è¡Œåå‘ä¼ æ’­ï¼Œå³ä¼šä¿®æ”¹å½“å‰çš„policy_modelå’Œvalue_model</li>
</ol>
</li>
</ol>
</li>
<li>è¾“å‡ºå’Œä¿å­˜å„ç§æ¨¡å‹</li>
</ol>
</blockquote>
<blockquote>
<p><strong>local_rollout_forward_batch_size</strong>
æ˜¯æ¯ä¸ªèŠ‚ç‚¹ï¼Œåœ¨æœ¬åœ°è¿›è¡Œå¤šæ‰¹æ¬¡çš„è®­ç»ƒï¼Œæ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°å³ä¸º local_rollout_forward_batch_sizeã€‚</p>
</blockquote>
<blockquote>
<p><strong>policy_modelå’Œref_modelåœ¨rolloutä¸­è°ƒç”¨çš„æ–¹æ³•ä¸åŒ</strong>
3. policy_modelä½¿ç”¨trl.trainer.utils.batch_generationï¼ˆæ˜¯å¯¹æ‰€æœ‰è®¡ç®—èŠ‚ç‚¹çš„å¹¶è¡Œæ‰¹é‡åŒ–è®¡ç®—ï¼‰ã€‚å°±æ˜¯åœ¨queryä¹‹åæ‹¼æ¥é¢„æµ‹å‡ºæ¥çš„responseã€‚
1. batch_generation ä½¿ç”¨generation_configå‚æ•°ï¼Œä»£ç ä¸­è§„å®š max_new_tokens=args.response_lengthï¼Œå³ç¡¬æ€§æŒ‡å®šäº†responseé•¿åº¦ã€‚
2. responseåœ¨ç”Ÿæˆå‡ºæ¥ä¹‹åï¼Œä¼šåˆå¹¶æ‰€æœ‰çš„ç«‹é©¬è¿›è¡Œå³paddingã€‚
3. å…¶ä¸­è°ƒç”¨äº†GenerationMixinçš„generateæ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„è¾“å‡ºä¸ºtoken idsçš„åºåˆ—ã€‚
4. ref_model ä½¿ç”¨ forwardã€‚å°±æ˜¯å°†æ•´ä¸ªquery_responseæ‹¼æ¥ç»“æœå…¨éƒ¨è¾“å…¥åˆ°ref_modelï¼Œä¸€æ¬¡æ€§å¾—å‡ºé”™ä½çš„é¢„æµ‹ç»“æœã€‚æ‰€ä»¥å¯¹logitsç´¢å¼•çš„æ—¶å€™ä¼šå¾€å‰é”™ä¸€ä½ï¼Œå¹¶ä¸”æœ€åä¸€ä¸ªä½ç½®æ˜¯ä¸éœ€è¦ä½¿ç”¨çš„ã€‚
1. æˆ‘æƒ³batch_generationæ˜¯éœ€è¦è¿›è¡Œå¤æ‚paddingçš„ï¼Œä»è€Œå¯èƒ½å¯¼è‡´æ¯ä¸€ä¸ªå°batchç”Ÿæˆçš„è¾“å‡ºé•¿åº¦æ˜¯ä¸ä¸€è‡´çš„ã€‚ä½†æˆ‘æƒ³ä¸åˆ°ä¸ºä»€ä¹ˆè¿™ä¹ˆè°ƒç”¨çš„ç†ç”±ã€‚
5. æ€»ä½“ä¸ŠPPOæœ‰ä¸¤ä¸ªåœ°æ–¹ç‰µæ¶‰åˆ°æ¦‚ç‡çš„å¯¹æ¯”
1. ç¬¬ä¸€ä¸ªæ˜¯ policy_modelå’Œref_modelä¸¤ä¸ªæ¨¡å‹çš„KLæ•£åº¦ï¼Œæ”¾ç½®policy_modelè·‘å¤ªè¿œè¿‡äºç¦»è°±ã€‚
2. ç¬¬äºŒä¸ªæ˜¯ policy_modelå’Œold_policy_modelçš„æ ·æœ¬æ¦‚ç‡å¯¹æ¯”ï¼Œä½†ä¸éœ€è¦ä¿ç•™old_policy_oldè¿™ä¸ªæ¨¡å‹ï¼Œå› ä¸ºè®­ç»ƒä¸­ä»…ä»…æ˜¯ä½¿ç”¨äº† old_policy_modelçš„æ ·æœ¬å’Œå…¶æ ·æœ¬æ¦‚ç‡ï¼Œæ‰€ä»¥é€šè¿‡old_policy_modelä¸€æ¬¡é‡‡æ ·å‡ºæ¥ä¸€å¤§å †æ•°æ®ä¹‹åï¼ˆå…¶ä¸­ä¿ç•™äº†æ ·æœ¬æ¦‚ç‡ $\Large p_{theta_{old}}$ï¼‰ï¼Œ old_policy_modelå°±å¯ä»¥ä¸¢å¼ƒï¼Œä»…ä»…é€šè¿‡ä½¿ç”¨é‡‡æ ·çš„æ ·æœ¬å’Œæ ·æœ¬æ¦‚ç‡æ¥è¿›è¡Œpolicy_modelçš„è®­ç»ƒä¼˜åŒ–ã€‚ é‚£ä¹ˆå°±ç›¸å½“äºæ˜¯ä¸¤ä¸ªæ¨¡å‹åˆäºŒä¸ºä¸€ã€‚</p>
</blockquote>
<blockquote>
<p><strong>class Qwen2ForSequenceClassification(Qwen2PreTrainedModel):</strong>
The Qwen2 Model transformer with a sequence classification head on top (linear layer).<br>
[<code>Qwen2ForSequenceClassification</code>] uses the <strong>last token</strong> in order to do the classification, as other causal models  (e.g. GPT-2) do.
è®¡ç®—çš„æ—¶å€™ä¼šæ‰¾åˆ° responseä¸­çš„ last_non_pad_tokenï¼Œè¾“å‡ºå¯¹åº”çš„logitisï¼Œç„¶åç»è¿‡scoreæ–¹æ³• è¿›è¡Œlinear_unit è®¡ç®—ã€‚</p>
</blockquote>
<blockquote>
<p><strong>class AlbertForSequenceClassification(AlbertPreTrainedModel):</strong>
Albert Model transformer with a sequence classification/regression head on top (a linear layer on top of the <strong>pooled  output</strong>) e.g. for GLUE tasks.</p>
</blockquote>
<blockquote>
<p><code>dataclass</code>Â æ³¨è§£æ˜¯ Python 3.7 åŠä»¥ä¸Šå¼•å…¥çš„ä¸€ä¸ªè£…é¥°å™¨ï¼Œä½œç”¨æ˜¯<strong>ç®€åŒ–ç±»çš„ç¼–å†™ï¼Œè®©ç±»è‡ªåŠ¨è·å¾—ä¸€äº›å¸¸ç”¨æ–¹æ³•</strong>ï¼ˆå¦‚Â <code>__init__</code>,Â <code>__repr__</code>,Â <code>__eq__</code>Â ç­‰ï¼‰ï¼Œç”¨äºè¡¨ç¤ºæ•°æ®ç»“æ„ã€‚</p>
</blockquote>
<blockquote>
<p>ä¸ºä»€ä¹ˆ è®¡ç®—ref_logprob ä½¿ç”¨ selective_log_softmaxæ–¹æ³•ï¼ŒæŒ‰ç†è¯´KLæ•£åº¦åº”è¯¥æ˜¯å¯¹vocabularyçš„æ‰€æœ‰è¯è¿›è¡ŒKLæ•£åº¦è®¡ç®—å•Šï¼Ÿ
<strong>ç†è®ºä¸Š</strong>ï¼ŒKLæ•£åº¦çš„å®šä¹‰æ˜¯ï¼š $$K L \left(\right. p \parallel q \left.\right) = \underset{i}{\sum} p \left(\right. i \left.\right) log â¡ \frac{p \left(\right. i \left.\right)}{q \left(\right. i \left.\right)}$$ è¿™é‡Œ (i) æ˜¯<strong>æ•´ä¸ª vocabulary</strong> çš„æ‰€æœ‰ tokenã€‚
<strong>å®é™…å·¥ç¨‹å®ç°ï¼ˆRLHF/PPOåœºæ™¯ï¼‰</strong>ï¼š
6. æˆ‘ä»¬é€šå¸¸åªå…³å¿ƒæ¨¡å‹å®é™…â€œèµ°å‡ºçš„è·¯å¾„â€ï¼Œå³ç”Ÿæˆçš„ token åºåˆ—ã€‚
7. PPO/Reward Modeling é‡Œï¼ŒKLé¡¹æ˜¯ç”¨æ¥çº¦æŸæ–°æ¨¡å‹ï¼ˆpolicyï¼‰ä¸è¦åç¦»æ—§æ¨¡å‹ï¼ˆreference/policyï¼‰çš„è¡Œä¸ºï¼Œ<strong>åªéœ€è¦å¯¹â€œå·²é‡‡æ ·çš„ tokenâ€ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒåšçº¦æŸ</strong>ã€‚</p>
</blockquote>
<blockquote>
<p><code>rewards[[actual_start, actual_end]] += scores</code>
èµ‹å€¼æ“ä½œï¼Œrewardsä¸­å­˜æ”¾çš„æ˜¯klæ•£åº¦ï¼Œè¿™é‡Œåˆ™æ˜¯å°†klæ•£åº¦å¯¹åº”æœ€åä¸€ä¸ªtokenä½ç½®å‘å³é”™ä¸€ä½çš„ä½ç½®åŠ ä¸Šä¸€ä¸ªæœ€ç»ˆçš„reward scoreã€‚
è¿™é‡Œçš„actual_startæ˜¯æ ·æœ¬çš„indexï¼Œactual_endæ˜¯æŒ‡æ¯ä¸€ä¸ªæ ·æœ¬çš„æœ€åä¸€ä¸ªtokenä½ç½®+1ã€‚</p>
</blockquote>
<blockquote>
<p><strong>åœ¨ Hugging Face Transformers ä¸­ï¼Œç±»åå‰çš„Â <code>Auto</code>Â è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©æ¨¡å‹çš„æ„æ€ã€‚</strong>
8. <code>AutoModelForSequenceClassification</code>Â ä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„æ¨¡å‹ç±»ï¼Œè€Œæ˜¯ä¸€ä¸ª<strong>å·¥å‚ç±»</strong>ã€‚
9. å®ƒå¯ä»¥æ ¹æ®ä½ åŠ è½½çš„ checkpointï¼ˆå¦‚Â <code>&quot;bert-base-uncased&quot;</code>ã€<code>&quot;roberta-base&quot;</code>ã€è‡ªå®šä¹‰è·¯å¾„ç­‰ï¼‰ï¼Œ<strong>è‡ªåŠ¨å®ä¾‹åŒ–å¯¹åº”çš„å…·ä½“æ¨¡å‹ç±»</strong>ï¼ˆå¦‚Â <code>BertForSequenceClassification</code>ã€<code>RobertaForSequenceClassification</code>Â ç­‰ï¼‰ã€‚
10. è¿™è®©ä½ ä¸éœ€è¦å…³å¿ƒåº•å±‚æ˜¯å“ªä¸ªæ¨¡å‹ï¼Œåªè¦ä¼ å…¥æ¨¡å‹åæˆ–è·¯å¾„ï¼Œå®ƒä¼šè‡ªåŠ¨å¸®ä½ é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹å®ç°ã€‚</p>
</blockquote>
<blockquote>
<p>PPOä¸­ value_modelä¸€èˆ¬ä½¿ç”¨çš„æ˜¯ sequence_classification modelï¼Œæ¯”å¦‚ <a href="https://github.com/huggingface/trl/blob/main/examples/scripts/ppo/ppo.py" target="_blank" rel="noopener" style="color:#42b983";>ç¤ºä¾‹è„šæœ¬</a> ä¸­ä»£ç å¦‚ä¸‹ï¼Œvalue_modelå’Œreward_modelä½¿ç”¨çš„æ˜¯åŒä¸€ä¸ªæ¨¡å‹ç±»çš„ä¸åŒå®ä¾‹ã€‚</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>value_model <span style="color:#5bc4bf">=</span> AutoModelForSequenceClassification<span style="color:#5bc4bf">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>    training_args<span style="color:#5bc4bf">.</span>reward_model_path, trust_remote_code<span style="color:#5bc4bf">=</span>model_args<span style="color:#5bc4bf">.</span>trust_remote_code, num_labels<span style="color:#5bc4bf">=</span><span style="color:#f99b15">1</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>reward_model <span style="color:#5bc4bf">=</span> AutoModelForSequenceClassification<span style="color:#5bc4bf">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>    training_args<span style="color:#5bc4bf">.</span>reward_model_path, trust_remote_code<span style="color:#5bc4bf">=</span>model_args<span style="color:#5bc4bf">.</span>trust_remote_code, num_labels<span style="color:#5bc4bf">=</span><span style="color:#f99b15">1</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>è¿™é‡Œä¸¾ä¾‹ <code>transformers.models.qwen2.modeling_qwen2.Qwen2ForSequenceClassification</code></p>
<blockquote>
<p>PPOTrainerä¸­è°ƒç”¨get_rewardçš„æ—¶å€™ï¼Œä½¿ç”¨çš„æ˜¯ sequence_classfication_modelçš„ base_model_prefixæŒ‡å‘çš„åº•å±‚LLMåŸå§‹æ¨¡å‹  ã€ scoreæ–¹æ³•ï¼ˆscoreæ–¹æ³•å°±æ˜¯åœ¨baseLLMæ¨¡å‹åæ·»åŠ ä¸€ä¸ªçº¿æ€§å±‚ï¼Œä»¥æ˜ å°„åˆ°logitsï¼‰ã€‚å…¶æ•ˆæœå°±æ˜¯å¯¹ä¸€å¯¹é—®ç­”å­—ç¬¦ä¸²åºåˆ—è¾“å‡ºä¸€ä¸²rewardæ•°å€¼ã€‚</p>
</blockquote>
<blockquote>
<p>get_rewardæ–¹æ³•ä¸­çš„å…·ä½“é€»è¾‘ã€‚
lm_backboneåº•å±‚ä½¿ç”¨çš„æ˜¯åŸºæ¨¡å‹ï¼Œå…¶ä¸­ output_hidden_states è¡¨ç¤ºè¦æŠŠæ¨¡å‹ä¸­æ‰€æœ‰å±‚çš„hidden_stateså…¨éƒ¨è¿›è¡Œè¾“å‡ºã€‚æ‰€ä»¥åœ¨è°ƒç”¨scoreçš„æ—¶å€™ä¼šåªå–æœ€åä¸€å±‚çš„hidden_statesï¼Œç„¶åè¾“å…¥åˆ°scoreæ–¹æ³•ä¸­ï¼ˆå³å†ç»è¿‡ä¸€å±‚çº¿æ€§å±‚å¾—åˆ°logitsï¼‰ã€‚æ•ˆæœå°±æ˜¯å¯¹äºé—®ç­”å­—ç¬¦ä¸²åºåˆ—ä¸­çš„<strong>æ¯ä¸€ä¸ªtoken</strong>éƒ½ä¼šå¾—åˆ°ä¸€ä¸ªreward_logitsæ•°å€¼ã€‚</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>output <span style="color:#5bc4bf">=</span> lm_backbone(  
</span></span><span style="display:flex;"><span>    input_ids<span style="color:#5bc4bf">=</span>input_ids,  
</span></span><span style="display:flex;"><span>    attention_mask<span style="color:#5bc4bf">=</span>attention_mask,  
</span></span><span style="display:flex;"><span>    position_ids<span style="color:#5bc4bf">=</span>position_ids,  
</span></span><span style="display:flex;"><span>    return_dict<span style="color:#5bc4bf">=</span><span style="color:#815ba4">True</span>,  
</span></span><span style="display:flex;"><span>    output_hidden_states<span style="color:#5bc4bf">=</span><span style="color:#815ba4">True</span>,  
</span></span><span style="display:flex;"><span>    use_cache<span style="color:#5bc4bf">=</span><span style="color:#815ba4">False</span>,  <span style="color:#776e71"># otherwise mistral-based RM would error out  </span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>reward_logits <span style="color:#5bc4bf">=</span> model<span style="color:#5bc4bf">.</span>score(output<span style="color:#5bc4bf">.</span>hidden_states[<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1</span>])
</span></span><span style="display:flex;"><span><span style="color:#776e71"># è¿™é‡Œçš„output.hidden_states è¡¨ç¤ºbase_modelçš„æ‰€æœ‰å±‚çš„è¾“å‡º</span>
</span></span><span style="display:flex;"><span><span style="color:#776e71"># output.hidden_states[-1] åˆ™è¡¨ç¤ºæœ€åä¸€å±‚çš„è¾“å‡º</span>
</span></span><span style="display:flex;"><span><span style="color:#776e71"># Qwen2ForSequenceClassification çš„forwardä¸­è®¡ç®— lossï¼Œå°±æ˜¯æŒ‰ç…§æœ€åä¸€ä¸ªtokençš„æœ€åä¸€å±‚è¾“å‡º +  çœŸå®label ä¸€èµ·è®¡ç®—å‡ºäº¤å‰ç†µ</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#815ba4">return</span> (  
</span></span><span style="display:flex;"><span>    reward_logits,  
</span></span><span style="display:flex;"><span>    reward_logits[  
</span></span><span style="display:flex;"><span>        torch<span style="color:#5bc4bf">.</span>arange(reward_logits<span style="color:#5bc4bf">.</span>size(<span style="color:#f99b15">0</span>), device<span style="color:#5bc4bf">=</span>reward_logits<span style="color:#5bc4bf">.</span>device),  
</span></span><span style="display:flex;"><span>        sequence_lengths,  
</span></span><span style="display:flex;"><span>    ]<span style="color:#5bc4bf">.</span>squeeze(<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1</span>),  
</span></span><span style="display:flex;"><span>    sequence_lengths,  
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#776e71"># è¿™é‡Œè¡¨ç¤ºè¿”å› responseåºåˆ—ä¸­æœ€åä¸€ä¸ªåˆæ³•tokençš„logitsè¾“å‡º</span>
</span></span></code></pre></div><p><code>transformers.models.qwen2.modeling_qwen2.Qwen2ForSequenceClassification</code> æ˜¯ä¸€ä¸ªä¾‹å­æ¨¡å‹ï¼Œè¯¥æ¨¡å‹çš„forwardæ–¹æ³•ä¸­è®¡ç®—äº†æ¯ä¸€ä¸ªtokenè¾“å‡ºçš„åˆ†ç±»çš„logitsï¼Œç„¶åä»…ä»…è·å–äº†æ¯ä¸€ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ª non_padding_tokençš„logitsä½œä¸ºè¾“å‡ºã€‚</p>
<hr>
<p>reward_modelå¾—åˆ°çš„ä¸€ä¸ªåºåˆ—ä¸€ä¸ªrewardå€¼ï¼Œä½†å…¶å®æ˜¯æœ€åä¸€æ­¥çš„immediate rewardã€‚
value_modelå¾—åˆ°çš„æ¯ä¸€ä¸ªåŠ¨ä½œï¼ˆå³tokenï¼‰ä¸€ä¸ªvalueå€¼ï¼ˆå³é•¿æœŸæ•ˆæœçš„è¯„ä¼°çš„æŒ‡æ ‡ï¼‰ã€‚</p>
<hr>
<p><strong>PPOTrainerä¸­ ä¸ºä»€ä¹ˆéœ€è¦æœ‰ missing_eos_penaltyï¼Ÿ</strong></p>
<p>åœ¨Â <code>PPOTrainer</code>Â ä¸­å­˜åœ¨Â <code>missing_eos_penalty</code>ï¼Œä¸»è¦æ˜¯ä¸ºäº†è§£å†³Â <strong>ç”Ÿæˆç»“æœæ²¡æœ‰åŒ…å«ç»ˆæ­¢ç¬¦ï¼ˆå¦‚Â <code>eos_token_id</code>ï¼‰çš„æƒ…å†µ</strong>ï¼Œé˜²æ­¢æ¨¡å‹ç”Ÿæˆä¸å®Œæ•´æˆ–å¼‚å¸¸çš„å“åº”ã€‚</p>
<ul>
<li>åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ¨¡å‹é€šå¸¸ä¼šåœ¨å“åº”ç»“å°¾ç”Ÿæˆä¸€ä¸ªâ€œç»ˆæ­¢ç¬¦â€ï¼ˆä¾‹å¦‚Â <code>eos_token_id</code>ï¼‰ï¼Œè¡¨ç¤ºå“åº”ç»“æŸã€‚</li>
<li>å¦‚æœæ¨¡å‹æ²¡æœ‰ç”Ÿæˆç»ˆæ­¢ç¬¦ï¼Œå“åº”å¯èƒ½ï¼š
<ul>
<li>è¶…é•¿ï¼ˆä¸€ç›´ç”Ÿæˆåˆ°æœ€å¤§é•¿åº¦ï¼‰</li>
<li>ä¸å®Œæ•´ï¼ˆç¼ºå°‘è¯­æ³•ä¸Šçš„ç»“å°¾ï¼‰</li>
<li>å½±å“åç»­è¯„ä¼°å’Œè®­ç»ƒï¼ˆå¦‚å¥–åŠ±æ¨¡å‹ã€PPOç­‰ï¼‰</li>
</ul>
</li>
<li>å¦‚æœå“åº”æ²¡æœ‰ç»ˆæ­¢ç¬¦ï¼Œå¯èƒ½æ˜¯æ¨¡å‹æ²¡å­¦ä¼šâ€œä½•æ—¶ç»“æŸâ€ï¼Œè¿™ç§å“åº”ä¸€èˆ¬æ˜¯ä¸ç¬¦åˆä»»åŠ¡é¢„æœŸçš„ï¼Œ<strong>éœ€è¦æƒ©ç½š</strong>ã€‚</li>
<li><code>missing_eos_penalty</code>Â å°±æ˜¯å¯¹è¿™ç§æƒ…å†µåŠ ä¸€ä¸ªè´Ÿåˆ†ï¼Œé¼“åŠ±æ¨¡å‹åœ¨åˆé€‚çš„æ—¶å€™ç”Ÿæˆç»ˆæ­¢ç¬¦ã€‚</li>
</ul>
<hr>
<blockquote>
<p><a href="http://joschu.net/blog/kl-approx.html" target="_blank" rel="noopener" style="color:#42b983";>Approximating KL Divergence</a></p>
</blockquote>
<blockquote>
<p>PPOTrainerä¸­ä¸ºä»€ä¹ˆè¦whiten_rewards
åœ¨Â <code>PPOTrainer</code>Â ä¸­ï¼Œå­˜åœ¨ä¸€ä¸ªå‚æ•°Â <code>whiten_rewards</code>ï¼Œå…¶ä½œç”¨æ˜¯å¯¹å¥–åŠ±ï¼ˆrewardï¼‰è¿›è¡Œå½’ä¸€åŒ–/æ ‡å‡†åŒ–ï¼ˆwhiteningï¼‰ã€‚
å¥–åŠ±çš„å°ºåº¦å’Œåˆ†å¸ƒç›´æ¥å½±å“ä¼˜åŠ¿çš„åˆ†å¸ƒï¼Œè€Œä¼˜åŠ¿åˆ†å¸ƒåˆå½±å“æ¢¯åº¦æ›´æ–°çš„ç¨³å®šæ€§å’Œè®­ç»ƒé€Ÿåº¦ã€‚
å¦‚æœå¥–åŠ±å¾ˆå¤§æˆ–å¾ˆå°ï¼Œä¼šå¯¼è‡´ç­–ç•¥æ¢¯åº¦å¾ˆå¤§/å¾ˆå°ï¼Œå½±å“æ”¶æ•›é€Ÿåº¦ï¼Œç”šè‡³å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±ã€‚</p>
</blockquote>
<blockquote>
<p>é‚£ä¹ˆåœ¨é‡‡æ ·é˜¶æ®µè®¡ç®—çš„rewardã€ valueç­‰æ•°æ®ï¼Œå› ä¸ºåœ¨è®­ç»ƒé˜¶æ®µè¿™äº›æ•°æ®ä¸­çš„å¾ˆå¤šä¼šé‡æ–°ç”Ÿæˆï¼Œæ‰€ä»¥é‡‡æ ·é˜¶æ®µå¾ˆå¤šè®¡ç®—æ˜¯ä¸æ˜¯æµªè´¹äº†?</p>
<ul>
<li><strong>é‡‡æ ·é˜¶æ®µçš„ value/logprob</strong>åªç”¨ä¸€æ¬¡ï¼Œè®­ç»ƒé˜¶æ®µä¼šé‡æ–°ç®—â€œæ–°ç­–ç•¥â€çš„ value/logprobã€‚</li>
</ul>
</blockquote>
<hr>
<div class="highlight"><pre tabindex="0" style="color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#815ba4">for</span> t <span style="color:#5bc4bf">in</span> reversed(range(gen_length)):  
</span></span><span style="display:flex;"><span>    nextvalues <span style="color:#5bc4bf">=</span> values[:, t <span style="color:#5bc4bf">+</span> <span style="color:#f99b15">1</span>] <span style="color:#815ba4">if</span> t <span style="color:#5bc4bf">&lt;</span> gen_length <span style="color:#5bc4bf">-</span> <span style="color:#f99b15">1</span> <span style="color:#815ba4">else</span> <span style="color:#f99b15">0.0</span>  
</span></span><span style="display:flex;"><span>    delta <span style="color:#5bc4bf">=</span> rewards[:, t] <span style="color:#5bc4bf">+</span> args<span style="color:#5bc4bf">.</span>gamma <span style="color:#5bc4bf">*</span> nextvalues <span style="color:#5bc4bf">-</span> values[:, t]  
</span></span><span style="display:flex;"><span>    lastgaelam <span style="color:#5bc4bf">=</span> delta <span style="color:#5bc4bf">+</span> args<span style="color:#5bc4bf">.</span>gamma <span style="color:#5bc4bf">*</span> args<span style="color:#5bc4bf">.</span>lam <span style="color:#5bc4bf">*</span> lastgaelam  
</span></span><span style="display:flex;"><span>    advantages_reversed<span style="color:#5bc4bf">.</span>append(lastgaelam)  
</span></span><span style="display:flex;"><span>advantages <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>stack(advantages_reversed[::<span style="color:#5bc4bf">-</span><span style="color:#f99b15">1</span>], axis<span style="color:#5bc4bf">=</span><span style="color:#f99b15">1</span>)  
</span></span><span style="display:flex;"><span>returns <span style="color:#5bc4bf">=</span> advantages <span style="color:#5bc4bf">+</span> values  
</span></span><span style="display:flex;"><span>advantages <span style="color:#5bc4bf">=</span> masked_whiten(advantages, <span style="color:#5bc4bf">~</span>padding_mask)  
</span></span><span style="display:flex;"><span>advantages <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>masked_fill(advantages, padding_mask, <span style="color:#f99b15">0</span>)
</span></span></code></pre></div><p>è¿™é‡Œè®¡ç®—returnå’Œadvantageã€‚returnå°±æ˜¯ state valueçš„ä¸€ä¸ªçœŸå®é‡‡æ ·ï¼Œç”¨æ¥è®¡ç®— value_lossï¼Œå³returnå’Œä¼°è®¡çš„valueä¹‹é—´çš„å¹³æ–¹æŸå¤±ã€‚advantage å°±æ˜¯ ä¼˜åŠ¿ï¼Œå³å½“å‰æ—¶åˆ»ä»¥åŠæ¯ä¸ªä¹‹åçš„æ—¶åˆ»æ‰€è®¡ç®—çš„advantageçš„æ‰“æŠ˜ä¹‹å’Œï¼Œç”¨æ¥ç»™ç­–ç•¥æ¢¯åº¦åŠ æƒã€‚</p>
<hr>
<p>æŸå¤±å‡½æ•°</p>
<div class="highlight"><pre tabindex="0" style="color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>logprobs_diff <span style="color:#5bc4bf">=</span> new_logprobs <span style="color:#5bc4bf">-</span> mb_logprobs  
</span></span><span style="display:flex;"><span>ratio <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>exp(logprobs_diff)  
</span></span><span style="display:flex;"><span>pg_losses <span style="color:#5bc4bf">=</span> <span style="color:#5bc4bf">-</span>mb_advantage <span style="color:#5bc4bf">*</span> ratio  
</span></span><span style="display:flex;"><span>pg_losses2 <span style="color:#5bc4bf">=</span> <span style="color:#5bc4bf">-</span>mb_advantage <span style="color:#5bc4bf">*</span> torch<span style="color:#5bc4bf">.</span>clamp(ratio, <span style="color:#f99b15">1.0</span> <span style="color:#5bc4bf">-</span> args<span style="color:#5bc4bf">.</span>cliprange, <span style="color:#f99b15">1.0</span> <span style="color:#5bc4bf">+</span> args<span style="color:#5bc4bf">.</span>cliprange)  
</span></span><span style="display:flex;"><span>pg_loss_max <span style="color:#5bc4bf">=</span> torch<span style="color:#5bc4bf">.</span>max(pg_losses, pg_losses2)  
</span></span><span style="display:flex;"><span>pg_loss <span style="color:#5bc4bf">=</span> masked_mean(pg_loss_max, <span style="color:#5bc4bf">~</span>padding_mask[micro_batch_inds])  
</span></span><span style="display:flex;"><span>loss <span style="color:#5bc4bf">=</span> pg_loss <span style="color:#5bc4bf">+</span> args<span style="color:#5bc4bf">.</span>vf_coef <span style="color:#5bc4bf">*</span> vf_loss
</span></span></code></pre></div><p>è¿™å—ä»£ç ä¸­ä¸ºä»€ä¹ˆæ²¡æœ‰ä½¿ç”¨ logpçš„å¯¼æ•°ï¼ŒåŸå› æ˜¯  $\Large \mathbb{E}<em>{\beta} \left[ \frac{\pi</em>{\theta}(a|s)}{\beta(a|s)} Q^{\pi}(s,a)\nabla_{\theta} \ln \pi_{\theta}(a|s) \right]$ ï¼Œå…¶å®å°±æ˜¯ $\Large \mathbb{E}<em>{\beta} \left[ \frac{\nabla</em>{\theta} \pi_{\theta}(a|s)}{\beta(a|s)} Q^{\pi}(s,a) \right]$ï¼Œé‚£ä¹ˆå¯¹åº”çš„æŸå¤±å‡½æ•°å°±æ˜¯ä»¥$\large \theta$ä¸ºä¼˜åŒ–å‚æ•°çš„  $\Large - \mathbb{E}<em>{\beta} \left[ \frac{\pi</em>{\theta}(a|s)}{\beta(a|s)} Q^{\pi}(s,a) \right]$ã€‚</p>
<hr>
<p>mb_logprobs åœ¨lossä¸­ä¸ä¼šåå‘ä¼ æ’­æ¢¯åº¦ä¹ˆ
å› ä¸º mb_logprobsè®¡ç®—çš„æ—¶å€™æ˜¯åœ¨ <code>with torch.no_grad():</code> ä¸­ã€‚</p>
<hr>
<p><code>vf_losses1 = torch.square(vpred - mb_return)</code> è¿™é‡Œçš„mb_return å°±æ˜¯åç»­å¤šæ­¥ç´¯ç§¯çš„rewardå€¼ã€‚</p>
<hr>
<p>ä¸ºä»€ä¹ˆpolicy loss å’Œ value function loss è¦åŠ èµ·æ¥è¿›è¡Œoptimizeï¼Ÿ</p>
<ol>
<li>å¦‚æœä½ è®¾è®¡äº†<strong>åˆ†ç¦»çš„ policy model å’Œ value model</strong>ï¼ˆå³ Actor å’Œ Critic å®Œå…¨åˆ†ç¦»ï¼‰ï¼Œé‚£ä¹ˆç¡®å®å¯ä»¥åˆ†å¼€ optimizeã€åˆ†å¼€ backwardã€‚</li>
<li>ä½†æœ€ä¸»æµçš„å®ç°ï¼ˆæ¯”å¦‚ Huggingface Transformers çš„ PPOTrainerï¼‰æ˜¯<strong>åˆä¸€æ¨¡å‹</strong>ï¼Œä¸€ä¸ªæ¨¡å‹é‡Œæœ‰ä¸¤ä¸ªè¾“å‡º headï¼Œå‚æ•°æ˜¯å…±äº«çš„ï¼Œæ‰€ä»¥å¿…é¡»æŠŠ loss åˆåœ¨ä¸€èµ·ï¼Œç»Ÿä¸€ backward å’Œ optimizeã€‚</li>
<li>ä¼˜åŒ–å™¨ï¼ˆoptimizerï¼‰ä¼šå¯¹æ‰€æœ‰å‚æ•°åšæ¢¯åº¦æ›´æ–°ï¼Œpolicy head å’Œ value headçš„æ¢¯åº¦ä¼šåˆ†åˆ«å›ä¼ åˆ°ä¸»å¹²å’Œå„è‡ª headã€‚</li>
</ol>
<hr>
<p>åœ¨train æ–¹æ³•å†…ï¼Œ<code>ref_policy</code> æ˜¯è‡ªå§‹è‡³ç»ˆ ä¸€è‡´ä¿æŒä¸å˜çš„ã€‚<code>ref_policy </code>åªæ˜¯é¿å… <code>policy_model</code> è·‘å¾—å¤ªè¿œã€‚è€Œæ ·æœ¬é‡ç”¨æ˜¯ä½¿ç”¨importantce weightæ¥è§£å†³çš„ï¼Œå³ $\large \mathbb{E}<em>{\beta} \left[ \frac{ \pi</em>{\theta}(a|s)}{\beta(a|s)} \right]$  ã€‚</p>
<hr>
<p><strong>è¿™ä¸ªPPOç®—æ³•çš„å®ç°ï¼Œä¸ºä»€ä¹ˆæ—¢æœ‰klæ•£åº¦è®¡ç®—ä½œä¸º rewardï¼ŒåŒæ—¶åˆæœ‰clipæ“ä½œ</strong></p>
<ul>
<li><strong>clip objective</strong>ï¼šæ§åˆ¶å•æ¬¡æ›´æ–°ä¸è¦å¤ªå¤§ï¼Œé˜²æ­¢è®­ç»ƒä¸ç¨³å®šã€‚</li>
<li><strong>KL æ•£åº¦å¥–åŠ±</strong>ï¼šä¿è¯æ•´ä½“ç­–ç•¥ä¸ä¼šé€æ¸åç¦»å‚è€ƒæ¨¡å‹ï¼ˆhuman-preference-aligned policyï¼‰å¤ªè¿œã€‚</li>
<li>KL æ•£åº¦ç›¸å½“äºåœ¨ <strong>å¥–åŠ±å±‚é¢</strong>æƒ©ç½šç­–ç•¥è¿œç¦»å‚è€ƒæ¨¡å‹ï¼Œé¿å…æ¨¡å‹è·‘åï¼›è¿™å°±æ˜¯æ‰€è°“çš„â€œKL å¥–åŠ±å¡‘å½¢â€ã€‚</li>
<li>clip è§£å†³ <strong>çŸ­æœŸè®­ç»ƒç¨³å®šæ€§</strong>ï¼›</li>
<li>KL å¥–åŠ±è§£å†³ <strong>é•¿æœŸåç§»é—®é¢˜</strong>ï¼Œç›¸å½“äºç»™æ¨¡å‹åŠ äº†ä¸ªâ€œç‰µå¼•ç»³â€ã€‚</li>
</ul>
<p>ä½ çœ‹åˆ°çš„å®ç°é‡Œï¼Œ<strong>KL æ•£åº¦ä¸æ˜¯ç›´æ¥å½“çº¦æŸç”¨ï¼Œè€Œæ˜¯ä½œä¸ºå¥–åŠ±é¡¹å‚ä¸å›æŠ¥è®¡ç®—</strong>ï¼›è€Œ <strong>clip åˆ™åœ¨ä¼˜åŒ–ç›®æ ‡é‡Œçº¦æŸç­–ç•¥æ›´æ–°</strong>ã€‚è¿™ä¸¤è€…æ˜¯äº’è¡¥å…³ç³»ã€‚</p>
<hr>
<hr>
<h4 id="dpo">DPO<a hidden class="anchor" aria-hidden="true" href="#dpo">#</a></h4>
<p>TRPOä¸­çš„POå’Œ DPOä¸­çš„POæŒ‡çš„ä¸æ˜¯ä¸€ä¸ªä¸œè¥¿ï¼Ÿ</p>
<ul>
<li><strong>TRPO</strong>ï¼šå®ƒçš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–<strong>ç¯å¢ƒå¥–åŠ±</strong>ã€‚<strong>TRPO</strong> (Trust Region <strong>Policy Optimization</strong>)</li>
<li><strong>DPO</strong>ï¼šå®ƒçš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–<strong>äººç±»åå¥½</strong>ã€‚ <strong>DPO</strong> (Direct Preference <strong>Optimization</strong>)</li>
</ul>
<hr>
<p>DPOçš„ç›´è§‰åŒ–ç†è§£</p>
<ol>
<li>æ•´ä½“çš„ç›®æ ‡å‡½æ•°ä¸º $$\large L(\theta) = \max_{\pi_\theta} \mathbb{E}<em>{x \sim D, y \sim \pi</em>\theta} [r(x, y)] - \beta \text{KL}[\pi_\theta(y|x), \pi_{\theta_{\text{old}}}(y|x)]$$ï¼Œå‡½æ•°è¡¨è¾¾çš„æ„ä¹‰æ˜¯ï¼Œåœ¨ä¸€ä¸ªå¥–åŠ±ç»“æ„ä¸Šï¼Œpolicyçš„ç»“æ„å¿…é¡»å°½é‡ä¸å¥–åŠ±ç»“æ„ä¿æŒä¸€è‡´ï¼Œä½†åŒæ—¶ä¸è¦åç¦»è€çš„policyå¤ªè¿œï¼Œåè€…å¯ä»¥è®¤ä¸ºæ˜¯ä¸€å®šç¨‹åº¦çš„æ­£åˆ™åŒ–ã€‚</li>
<li><strong>å½¢è±¡åŒ–ç†è§£</strong>ï¼Œå¯¹äº yå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒå’Œå¥–åŠ±åˆ†å¸ƒï¼ˆå³æ¨ªè½´æ˜¯y çš„å„ç§å–å€¼ï¼Œçºµè½´æ˜¯å¯¹åº”yå€¼è¡Œä¸ºçš„æ¦‚ç‡å€¼å¤§å°ã€å¥–åŠ±å¤§å°ï¼‰ï¼Œé‚£ä¹ˆç”¨å›¾å½¢ç†è§£å°±æ˜¯ä¸¤æ¡æ›²çº¿ï¼ˆåªä¸è¿‡ä¸€ä¸ªæ˜¯å½’ä¸€åŒ–çš„æ­£å€¼ï¼Œä¸€ä¸ªæ˜¯å¯èƒ½æ­£è´Ÿå€¼å‡å­˜åœ¨ï¼‰ã€‚å¯¹äºä¸Šè¿°ç›®æ ‡å‡½æ•°æ¥è¯´ï¼Œæš‚ä¸”è®¤ä¸ºå¥–åŠ±åˆ†å¸ƒ$r(x,y)$å°±æ˜¯çœŸå®çš„è®­ç»ƒæ•°æ®å€¼ï¼ˆä½†å…¶å®æ˜¯ä» æ­£è´Ÿä¾‹responseçš„ contrastive lossæ‹Ÿåˆå‡ºæ¥çš„å€¼ï¼‰ï¼Œé‚£ä¹ˆ$L(\theta)$ä¸­çš„å”¯ä¸€å˜é‡å°±æ˜¯ ç­–ç•¥å‡½æ•° $\pi_{\theta}$ ï¼Œè¿™ä¸ªæ—¶å€™ç›®æ ‡å‡½æ•°$L(\theta)$å…¶å®å°±æ˜¯ä¸€ä¸ªä»¥ $\pi_\theta$ä¸ºè‡ªå˜é‡çš„ä¸€ä¸ªå‡½æ•°ï¼ˆå½“ç„¶è‡ªå˜é‡æœ¬èº«å°±æ˜¯æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ï¼Œé‚£ä¹ˆè¿™å…¶å®å°±æ˜¯ä¸€ä¸ªæ³›å‡½ä¼˜åŒ–é—®é¢˜ï¼‰ï¼›å¯¹ä¸Šè¿°ç›®æ ‡å‡½æ•°è¿›è¡Œå˜æ¢ï¼Œå¯ä»¥å¾—å‡º $L(\theta)$å–å¾—æœ€å¤§å€¼å¯¹åº”çš„è‡ªå˜é‡ $\pi_\theta$ æ˜¯æœ‰ä¸€ä¸ªå›ºå®šå…¬å¼çš„ï¼Œå³ $$\large \pi_r(y|x) = \pi^*(y|x) = \frac{1}{Z(x)}\pi_{\theta_{\text{old}}}(y|x) \exp\left(\frac{1}{\beta}r(x,y)\right)$$ã€‚
<ol>
<li>è¿™é‡Œå¯ä»¥æç®€çš„æ–¹æ³•æ¨å¯¼å¤„ç†ï¼Œå¦‚ä¸‹ $$L =\mathbb{E}[r]-KL = \sum_{p} p*(r-\log(\frac{p}{q})) = \sum_{p} p*(\log(e^r)-\log(\frac{p}{q}))=\sum_{p} p*(\log(\frac{e^r<em>q}{p})) = - \sum_{p} p</em>(\log(\frac{e^r<em>q}{p})) = - KL(p, e^r</em>q)$$ï¼Œè€Œæœ€åä¸€ä¸ªå¼å­å°±æ˜¯KLæ•£åº¦çš„å…¬å¼å–è´Ÿæ•°ï¼ŒKLæ•£åº¦å…¬å¼å­˜åœ¨æœ€å°å€¼ï¼Œé‚£ä¹ˆ$L$å°±å­˜åœ¨æœ€å¤§å€¼ï¼Œæœ€ä¼˜ç‚¹å³ä¸º $\large p = e^r*q$ï¼Œæœ€åå…¬å¼å¤–å›´å¥—ä¸ªå½’ä¸€åŒ–å°±æ˜¯æœ€ç»ˆçš„æ¨ç†ç»“æœã€‚</li>
</ol>
</li>
<li>ç›´è§‰åŒ–çš„ç†è§£å°±æ˜¯  ç­–ç•¥å‡½æ•° -&gt; å’Œå¥–åŠ±ç»“æ„å¯¹åº”çš„æœ€ä¼˜ç­–ç•¥å‡½æ•° -&gt; å¾—åˆ°æœ€ä¼˜çš„æ•´ä½“ç›®æ ‡å€¼ã€‚è€Œå¥–åŠ±ç»“æ„å¿…é¡»ä¸çœŸå®çš„æ­£è´Ÿæ ·ä¾‹åå¥½ç»“æ„ä¸€è‡´ï¼Œæ‰€ä»¥æ•´ä½“ä¸Šå°±æ˜¯ å¥–åŠ±ç»“æ„å’Œç­–ç•¥å‡½æ•°æ˜¯ç»‘æ­»çš„æœ‰å›ºå®šå‡½æ•°å…³ç³»ï¼Œè€Œå¥–åŠ±ç»“æ„é€šè¿‡æŸå¤±å‡½æ•°ä¸çœŸå®æ­£è´Ÿæ ·ä¾‹åå¥½å¯¹é½ï¼Œé‚£ä¹ˆåå‘æ¥è¯´å°±æ˜¯ çœŸå®æ­£è´Ÿæ ·ä¾‹åå¥½-&gt;æŒ‡å¯¼å¥–åŠ±ç»“æ„è®¡ç®— -&gt;æŒ‡å¯¼å¯¹åº”çš„ç­–ç•¥å‡½æ•°ï¼Œ è€Œå¯¹åº”ç­–ç•¥å‡½æ•°å…¶å®æœ¬æ¥å°±åº”è¯¥è¾¾åˆ°ä¸€ç§æç«¯ç­–ç•¥ï¼ˆå³ä½¿å¾—å¥–åŠ±æœ€å¤§çš„responseçš„æ¦‚ç‡ç›´æ¥æ‹‰åˆ°æœ€å¤§å€¼1ï¼‰ï¼Œä½†å› ä¸ºæ­£åˆ™åŒ–çš„å­˜åœ¨ä½¿å¾— ç­–ç•¥å‡½æ•°æ˜¯ä»‹äº è€ç­–ç•¥å‡½æ•°å’Œ æç«¯ç­–ç•¥ ä¹‹é—´çš„ä¸­é—´ç­–ç•¥ã€‚</li>
<li>å‡å¦‚æ²¡æœ‰æ­£åˆ™åŒ–ï¼Œé‚£ä¹ˆæ•´ä¸ªè¿‡ç¨‹å°±æ˜¯æç«¯ç­–ç•¥ï¼Œè®©æ­£æ ·ä¾‹çš„ç”Ÿæˆæ¦‚ç‡æ— é™å¤§ï¼Œè´Ÿæ ·ä¾‹çš„ç”Ÿæˆæ¦‚ç‡æ— é™å°ï¼Œä¹Ÿå°±æ˜¯å¯¹åº”çš„logitsä¹‹å·®æ— é™å¤§ï¼Œé‚£ä¹ˆæ­£æ ·ä¾‹çš„æ¦‚ç‡ç›´æ¥ä¸º1ï¼Œè´Ÿæ ·ä¾‹çš„æ¦‚ç‡ç›´æ¥ä¸º0ï¼Œä½†è¿™æ ·å…¶å®å°±æ˜¯è¿‡æ‹Ÿåˆã€‚æ‰€ä»¥éœ€è¦æ­£åˆ™åŒ–æ¥é™åˆ¶ï¼Œé‚£ä¹ˆå°±æ˜¯é€šè¿‡ ç”¨æ¦‚ç‡çš„logitsæ¥è¡¨ç¤º $r(x,y)$ï¼ŒåŒæ—¶å‡å®šæŸå¤±ç»“æ„æ˜¯ è®© $r(x,y)$ å’Œ KLæ•£åº¦ç›´æ¥ç›¸åŠ ï¼Œæ¥ä½œä¸ºæœ€ç»ˆçš„ç›®æ ‡å‡½æ•°ã€‚</li>
</ol>
<p>ç›´è§‚ä¸Šç†è§£DPOçš„å…¬å¼ï¼Œå³è®©æ­£åå¥½çš„å¯¹åº”çš„ç­–ç•¥actionæ¦‚ç‡è¶Šå¤§å¥½ï¼Œè®©è´Ÿåå¥½å¯¹åº”çš„ç­–ç•¥actionæ¦‚ç‡è¶Šå°è¶Šå¥½ã€‚
$$\large L(\theta)=-\mathbb{E}<em>{(x,y^+,y^-) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi</em>{\theta}(y^+|x)}{\pi_{\theta_{old}}(y^+|x)}-\beta \log \frac{\pi_{\theta}(y^-|x)}{\pi_{\theta_{old}}(y^-|x)}\right)\right].$$
å…¶ä¸­$\large \hat{r}<em>\theta(x,y) = \beta \log \left( \frac{\pi</em>\theta(y|x)}{\pi_{\theta_{old}}(y|x)} \right)$ æ„æ€æ˜¯ï¼Œå¦‚æœå½“å‰ç­–ç•¥å¯¹åº”çš„é¢„ä¼°å¥–åŠ±æ˜¯å¤šå°‘ã€‚</p>
<hr>
<p>ç›´è§‰ä¸Šæˆ‘å°†DPOæ¯”ä½œæ˜¯ä¸€ä¸ªä¸‰ä¸ªé“æ†ä¸Šåˆ†åˆ«å¥—ç€ä¸€ä¸ªç¯ï¼Œä¸‰ä¸ªé“ç¯ä¹‹é—´ æœ‰ä¸¤ä¸ªç»³å­è¿æ¥ï¼Œä¼ ç»Ÿçš„policy gradientæ˜¯å°†é€šè¿‡æ‹‰æœ€ä¸‹é¢ä¸€ä¸ªç¯ï¼Œè®©ä¸Šé¢ä¸¤ä¸ªé—´æ¥è¿å¸¦ç€ç§»åŠ¨ã€‚è€ŒDPOæ˜¯å°†ä¸Šé¢ä¸¤ä¸ªç¯å›ºåŒ–æˆä¸€ä½“ï¼Œåªè¦ç§»åŠ¨æœ€ä¸‹é¢ä¸€ä¸ªï¼Œå°±èƒ½è¾¾åˆ°ç›´æ¥ç§»åŠ¨æœ€ä¸Šé¢é“ç¯çš„ç›®çš„ã€‚</p>
<p>DPOç®—æ³•çš„è®­ç»ƒæ ‡å‡†æ•°æ®ï¼Œéƒ½æ˜¯é…å¯¹çš„ã€‚</p>
<p>DPOç®—æ³•ä¸­æŸå¤±å‡½æ•°ä¸­çš„logpæŒ‡çš„æ˜¯ä¸æ˜¯æ•´æ¡episodeä¸­ç­”æ¡ˆçš„æ¦‚ç‡logï¼Œå³åœ¨ç»™å®špromptçš„æƒ…å†µä¸‹ï¼Œç»™å‡ºresponseçš„æ¯ä¸ªtokenæ¦‚ç‡çš„ä¹˜ç§¯ï¼Œä¹Ÿå°±æ˜¯logpçš„å’Œã€‚</p>
<hr>
<h4 id="grpo">GRPO<a hidden class="anchor" aria-hidden="true" href="#grpo">#</a></h4>
<ol>
<li>DeepSeekMath è®ºæ–‡çš„è§£è¯» [[2025Q3-è®ºæ–‡å­¦ä¹ æ—¥è®°#2025-07-16 DeepSeekMath]]</li>
<li>GRPO ä¸­ä½¿ç”¨äº†process supervision ä¿¡å·ï¼Œå³å•ä¸ªreponseçš„å¤šæ­¥æ¨ç†ï¼Œæ¯ä¸€æ­¥éƒ½æœ‰æ ‡æ³¨çš„rewardã€‚ç„¶å advantage å°±æ˜¯ å½’ä¸€åŒ–åçš„reward åœ¨tæ­¥ä¹‹åçš„ç´¯è®¡å’Œã€‚å³åŸºäºåŒä¸€ä¸ªpromptçš„æ‰€æœ‰responseçš„rewardçš„å‡å€¼å’Œæ–¹å·® è¿›è¡Œæ ‡å‡†åŒ–  $\Large \tilde{r}<em>i^{index(j)} = \frac{r_i^{index(j)} - \text{mean}(\mathbf{R})}{\text{std}(\mathbf{R})}$   ï¼Œç„¶åç´¯è®¡å’Œ $\Large \hat{A}</em>{i,t} = \sum_{index(j) \geq t} \tilde{r}_i^{index(j)}$ ï¼Œè¿™é‡Œtå°±æ˜¯æ¯ä¸€ä¸ªtokenï¼Œjæ˜¯æ¯ä¸€ä¸ªreasoning stepï¼Œå³æ¯ä¸€ä¸ªtokenå…¶rewardå°±æ˜¯è¯¥tokenä¹‹åæ‰€æœ‰å¯¹åº”äº reasoning step end tokençš„tokençš„å¥–åŠ±ä¹‹å’Œã€‚æˆ‘æƒ³è¿™é‡Œä¹‹æ‰€ä»¥æ²¡æœ‰ä½¿ç”¨$\gamma$ å¯èƒ½æ˜¯å› ä¸ºreasoning stepæ•°é‡æœ¬æ¥éƒ½æ˜¯æå…¶æœ‰é™çš„ï¼Œå¹¶ä¸”é‡‡æ ·åº”è¯¥ä¼šæ§åˆ¶å…¶æ•°é‡ã€‚</li>
<li>æ‰€ä»¥GRPOæ˜¯éœ€è¦ä½¿ç”¨ process reward model å¯¹responseçš„æ¯ä¸€ä¸ªæ¨ç†æ­¥éª¤è¿›è¡Œæ‰“åˆ†çš„ã€‚</li>
</ol>
<hr>
<p>The training data of the reward model is based on the rule judgment.
Data Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:
Rule (whether the answer is correct or not)
è®ºæ–‡é‡Œçš„å…¬å¼é‡Œæœ‰å¾ˆå¤šè®²ç©¶ã€‚</p>
<hr>
<p>åŸå§‹è®ºæ–‡ä¸­çš„å…¬å¼19</p>
<p>$\large \mathcal{J}<em>{\text{GRPO}}(\theta) = \mathbb{E}</em>{\mathbf{q} \sim P_{\text{sft}}(Q), {o_i}<em>{i=1}^G \sim \pi</em>{\theta_{old}}(O|q)}   \large \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left[ \frac{\pi_\theta(o_{i,t}|q, o_{i,&lt;t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,&lt;t})} \hat{A}<em>{i,t} - \beta \left( \frac{\pi</em>{ref}(o_{i,t}|q, o_{i,&lt;t})}{\pi_\theta(o_{i,t}|q, o_{i,&lt;t})} - \log \frac{\pi_{ref}(o_{i,t}|q, o_{i,&lt;t})}{\pi_\theta(o_{i,t}|q, o_{i,&lt;t})} - 1 \right) \right].$</p>
<p>$\large w_{i,t}(\theta) = \frac{\pi_\theta(y_{i,t}|x, y_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(y_{i,t}|x, y_{i,&lt;t})}$</p>
<h4 id="gspo">GSPO<a hidden class="anchor" aria-hidden="true" href="#gspo">#</a></h4>
<p><a href="https://arxiv.org/abs/2507.18071" target="_blank" rel="noopener" style="color:#42b983";>GSPO Group Sequence Policy Optimization</a></p>
<p>$\large J_{\text{GSPO}}(\theta) = \mathbb{E}<em>{x \sim D, {y_i}</em>{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|x)} \left[ \frac{1}{G}\sum_{i=1}^G \min\left(s_i(\theta)\hat{A}_i, \text{clip}(s_i(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_i\right) \right]$</p>
<p>$\large \hat{A}<em>i = \frac{r(x, y_i) - \text{mean}({r(x, y_j)}</em>{j=1}^G)}{\text{std}({r(x, y_j)}_{j=1}^G)}$</p>
<p>$\large s_i(\theta) = \left(\frac{\pi_\theta(y_i|x)}{\pi_{\theta_{\text{old}}}(y_i|x)}\right)^{\frac{1}{|y_i|}} = \exp\left(\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}\log\frac{\pi_\theta(y_{i,t}|x,y_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(y_{i,t}|x,y_{i,&lt;t})}\right)$</p>
<p><strong>GSPOå’ŒGRPOå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„åŒºåˆ«</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>GRPO</th>
<th>GSPO</th>
</tr>
</thead>
<tbody>
<tr>
<td>è®­ç»ƒæ•°æ®</td>
<td>process supervision</td>
<td>outcome supervision</td>
</tr>
<tr>
<td>é‡è¦æ€§é‡‡æ ·</td>
<td>åŸºäºæ¯ä¸€ä¸ªtoken</td>
<td>åŸºäºæ•´ä¸ªåºåˆ—ï¼Œæ‰€æœ‰tokené‡è¦æ€§çš„çš„å‡ ä½•å¹³å‡</td>
</tr>
</tbody>
</table>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://chesterwang.github.io/chester-blog/posts/2025-11-07-rag%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E8%AE%B0%E5%BD%95/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>ä¸ªäººRAGé¡¹ç›®å¼€å‘è®°å½•</span>
  </a>
</nav>

        </footer>
    </div>



<div>
    <div class="pagination__title">
        <span class="pagination__title-h" style="font-size: 20px;">ğŸ’¬è¯„è®º</span>
        <hr />
    </div>

    <div id="tcomment"></div>

    <script src="https://image.lvbibir.cn/js/1.6.40/twikoo.all.min.js">
    </script>
    

    

    <script>
        twikoo.init({
            envId: "https://twikoo.lvbibir.cn/", 
            el: "#tcomment",
            lang: 'zh-CN',
            
            path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
            
            
            
            
            
            
            
        });
    </script>

</div>
</article>
</main>


<footer class="footer">

    <a href="https://gohugo.io/" target="_blank">
        
        <img style="display: unset;" src="https://image.lvbibir.cn/blog/frame-hugo-blue.svg">
    </a>
    <a href="https://github.com/adityatelange/hugo-PaperMod" target="_blank">
        
        <img style="display: unset;" src="https://image.lvbibir.cn/blog/theme-papermod-lightgrey.svg">
    </a>
    <a href="https://cn.aliyun.com/" target="_blank">
        
        <img style="display: unset;" src="https://image.lvbibir.cn/blog/å›¾åºŠ-é˜¿é‡Œäº‘-orange.svg">
    </a>

    <br>

    <span id="runtime_span"></span>
    <script
        type="text/javascript">function show_runtime() { window.setTimeout("show_runtime()", 1000); X = new Date("11/11/2025 1:00:00"); Y = new Date(); T = (Y.getTime() - X.getTime()); M = 24 * 60 * 60 * 1000; a = T / M; A = Math.floor(a); b = (a - A) * 24; B = Math.floor(b); c = (b - B) * 60; C = Math.floor((b - B) * 60); D = Math.floor((c - C) * 60); runtime_span.innerHTML = "ç½‘ç«™å·²è¿è¡Œ" + A + "å¤©" + B + "å°æ—¶" + C + "åˆ†" + D + "ç§’" } show_runtime();</script>
    |
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container">
        
        æ€»è®¿å®¢æ•°:  <span id="busuanzi_value_site_uv"></span>
        |
        æ€»è®¿é—®é‡:  <span id="busuanzi_value_site_pv"></span>
    </span>

    <br>

    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 400 || document.documentElement.scrollTop > 400) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>


<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            
            
            
            
            
            
            
            
            
            
            
            
            
            

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {
            }
            ;
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild === container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName === "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

</body>

</html>
