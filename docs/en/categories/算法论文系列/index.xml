<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>算法论文系列 on Chester&#39;s Blog</title>
    <link>http://localhost:1313/chester-blog/en/categories/%E7%AE%97%E6%B3%95%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97/</link>
    <description>Recent content in 算法论文系列 on Chester&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 06 Feb 2022 17:15:23 +0000</lastBuildDate><atom:link href="http://localhost:1313/chester-blog/en/categories/%E7%AE%97%E6%B3%95%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>2022-02-06-Keras SGD 源码阅读</title>
      <link>http://localhost:1313/chester-blog/en/posts/2022-02-06-keras-sgd-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link>
      <pubDate>Sun, 06 Feb 2022 17:15:23 +0000</pubDate>
      
      <guid>http://localhost:1313/chester-blog/en/posts/2022-02-06-keras-sgd-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</guid>
      <description>摘要 1. 缘起 Keras代码中 nesterov SGD算法的实现看起来和一些论文、博客中的公式不相同。 2. 现象 核心代码块 # from keras code: keras.optimizer_v1.SGD for p, g, m in zip(params, grads, moments): v = self.momentum * m - lr * g # velocity self.updates.append(tf.compat.v1.assign(m, v)) if self.nesterov: new_p = p + self.momentum * v - lr * g else: new_p = p + v 3. 分析原因 3.1. 有人提出相同的问题 Question about Nesterov momentum implementation · Issue #14115 · keras-team/keras Nesterov based gradient descent · Issue #966 · keras-team/keras New PR for Nesterov change by the-moliver · Pull Request #47 ·</description>
    </item>
    
    <item>
      <title>算法论文系列（一） 梯度优化算法综述</title>
      <link>http://localhost:1313/chester-blog/en/posts/2022-02-06-%E7%AE%97%E6%B3%95%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0/</link>
      <pubDate>Sun, 06 Feb 2022 11:49:56 +0000</pubDate>
      
      <guid>http://localhost:1313/chester-blog/en/posts/2022-02-06-%E7%AE%97%E6%B3%95%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0/</guid>
      <description>An overview of gradient descent optimization algorithms. 参考资料 Ruder S. An overview of gradient descent optimization algorithms[J]. arXiv preprint arXiv:1609.04747, 2016.</description>
    </item>
    
  </channel>
</rss>
